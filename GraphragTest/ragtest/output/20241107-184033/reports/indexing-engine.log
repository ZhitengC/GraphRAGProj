18:40:33,621 graphrag.config.read_dotenv INFO Loading pipeline .env file
18:40:33,623 graphrag.index.cli INFO using default configuration: {
    "llm": {
        "api_key": "REDACTED, length 51",
        "type": "openai_chat",
        "model": "glm-4",
        "max_tokens": 2000,
        "temperature": 0.95,
        "top_p": 0.7,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:3000/v1",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "./",
    "reporting": {
        "type": "file",
        "base_dir": "inputs/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "inputs/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_embedding",
            "model": "embedding-2",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 1,
        "batch_max_tokens": 8000,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
18:40:33,624 graphrag.index.create_pipeline_config INFO skipping workflows 
18:40:33,626 graphrag.index.run INFO Running pipeline
18:40:33,626 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at inputs/artifacts
18:40:33,626 graphrag.index.input.load_input INFO loading input from root_dir=input
18:40:33,626 graphrag.index.input.load_input INFO using file storage for input
18:40:33,627 graphrag.index.storage.file_pipeline_storage INFO search input for files matching .*\.txt$
18:40:33,627 graphrag.index.input.text INFO found text files from input, found [('currentPrompt.txt', {})]
18:40:33,628 graphrag.index.input.text INFO Found 1 files, loading 1
18:40:33,629 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_final_covariates', 'create_summarized_entities', 'join_text_units_to_covariate_ids', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
18:40:33,629 graphrag.index.run INFO Final # of rows loaded: 1
18:40:33,719 graphrag.index.run INFO Running workflow: create_base_text_units...
18:40:33,719 graphrag.index.run INFO dependencies for create_base_text_units: []
18:40:33,721 datashaper.workflow.workflow INFO executing verb orderby
18:40:33,722 datashaper.workflow.workflow INFO executing verb zip
18:40:33,724 datashaper.workflow.workflow INFO executing verb aggregate_override
18:40:33,727 datashaper.workflow.workflow INFO executing verb chunk
18:40:33,818 datashaper.workflow.workflow INFO executing verb select
18:40:33,820 datashaper.workflow.workflow INFO executing verb unroll
18:40:33,823 datashaper.workflow.workflow INFO executing verb rename
18:40:33,825 datashaper.workflow.workflow INFO executing verb genid
18:40:33,827 datashaper.workflow.workflow INFO executing verb unzip
18:40:33,830 datashaper.workflow.workflow INFO executing verb copy
18:40:33,832 datashaper.workflow.workflow INFO executing verb filter
18:40:33,837 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:40:33,936 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
18:40:33,936 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
18:40:33,936 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
18:40:33,944 datashaper.workflow.workflow INFO executing verb entity_extract
18:40:33,946 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:3000/v1
18:40:33,950 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for glm-4: TPM=0, RPM=0
18:40:33,950 graphrag.index.llm.load_llm INFO create concurrency limiter for glm-4: 25
18:40:33,974 datashaper.workflow.workflow INFO executing verb merge_graphs
18:40:33,986 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
18:40:34,92 graphrag.index.run INFO Running workflow: create_final_covariates...
18:40:34,92 graphrag.index.run INFO dependencies for create_final_covariates: ['create_base_text_units']
18:40:34,92 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
18:40:34,101 datashaper.workflow.workflow INFO executing verb extract_covariates
18:40:34,116 datashaper.workflow.workflow INFO executing verb window
18:40:34,119 datashaper.workflow.workflow INFO executing verb genid
18:40:34,123 datashaper.workflow.workflow INFO executing verb convert
18:40:34,130 datashaper.workflow.workflow INFO executing verb rename
18:40:34,133 datashaper.workflow.workflow INFO executing verb select
18:40:34,135 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_covariates.parquet
18:40:34,244 graphrag.index.run INFO Running workflow: create_summarized_entities...
18:40:34,244 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
18:40:34,244 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
18:40:34,254 datashaper.workflow.workflow INFO executing verb summarize_descriptions
18:40:34,311 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
18:40:34,409 graphrag.index.run INFO Running workflow: join_text_units_to_covariate_ids...
18:40:34,409 graphrag.index.run INFO dependencies for join_text_units_to_covariate_ids: ['create_final_covariates']
18:40:34,410 graphrag.index.run INFO read table from storage: create_final_covariates.parquet
18:40:34,422 datashaper.workflow.workflow INFO executing verb select
18:40:34,426 datashaper.workflow.workflow INFO executing verb aggregate_override
18:40:34,428 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_covariate_ids.parquet
18:40:34,529 graphrag.index.run INFO Running workflow: create_base_entity_graph...
18:40:34,529 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
18:40:34,529 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
18:40:34,544 datashaper.workflow.workflow INFO executing verb cluster_graph
18:40:34,580 datashaper.workflow.workflow INFO executing verb select
18:40:34,581 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:40:34,688 graphrag.index.run INFO Running workflow: create_final_entities...
18:40:34,688 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:40:34,692 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
18:40:34,706 datashaper.workflow.workflow INFO executing verb unpack_graph
18:40:34,723 datashaper.workflow.workflow INFO executing verb rename
18:40:34,728 datashaper.workflow.workflow INFO executing verb select
18:40:34,733 datashaper.workflow.workflow INFO executing verb dedupe
18:40:34,738 datashaper.workflow.workflow INFO executing verb rename
18:40:34,743 datashaper.workflow.workflow INFO executing verb filter
18:40:34,757 datashaper.workflow.workflow INFO executing verb text_split
18:40:34,763 datashaper.workflow.workflow INFO executing verb drop
18:40:34,769 datashaper.workflow.workflow INFO executing verb merge
18:40:34,796 datashaper.workflow.workflow INFO executing verb text_embed
18:40:34,796 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:3000/v1
18:40:34,800 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for embedding-2: TPM=0, RPM=0
18:40:34,800 graphrag.index.llm.load_llm INFO create concurrency limiter for embedding-2: 25
18:40:34,807 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 302 inputs via 302 snippets using 302 batches. max_batch_size=1, max_tokens=8000
18:40:35,518 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6900000000023283. input_tokens=29, output_tokens=0
18:40:35,535 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,535 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,537 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7129999999997381. input_tokens=34, output_tokens=0
18:40:35,540 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7099999999991269. input_tokens=28, output_tokens=0
18:40:35,721 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,721 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,721 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,722 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,722 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,722 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,723 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,725 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8940000000002328. input_tokens=31, output_tokens=0
18:40:35,727 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9020000000018626. input_tokens=28, output_tokens=0
18:40:35,729 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9040000000022701. input_tokens=27, output_tokens=0
18:40:35,732 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9050000000024738. input_tokens=30, output_tokens=0
18:40:35,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9040000000022701. input_tokens=30, output_tokens=0
18:40:35,738 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9009999999980209. input_tokens=27, output_tokens=0
18:40:35,740 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9179999999978463. input_tokens=39, output_tokens=0
18:40:35,742 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,744 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8960000000006403. input_tokens=32, output_tokens=0
18:40:35,924 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,924 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,924 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,927 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0790000000015425. input_tokens=28, output_tokens=0
18:40:35,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0820000000021537. input_tokens=33, output_tokens=0
18:40:35,931 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0780000000013388. input_tokens=30, output_tokens=0
18:40:35,940 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,940 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:35,942 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0930000000007567. input_tokens=29, output_tokens=0
18:40:35,944 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0970000000015716. input_tokens=29, output_tokens=0
18:40:36,135 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:36,136 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:36,136 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:36,138 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6179999999985739. input_tokens=27, output_tokens=0
18:40:36,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6000000000021828. input_tokens=26, output_tokens=0
18:40:36,143 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.603000000002794. input_tokens=29, output_tokens=0
18:40:36,151 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:36,152 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:36,154 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2819999999992433. input_tokens=30, output_tokens=0
18:40:36,156 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3029999999998836. input_tokens=30, output_tokens=0
18:40:36,184 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:36,187 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.326999999997497. input_tokens=26, output_tokens=0
18:40:36,339 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:36,341 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6000000000021828. input_tokens=26, output_tokens=0
18:40:36,357 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:36,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4989999999997963. input_tokens=27, output_tokens=0
18:40:36,369 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:36,369 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:36,369 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:36,369 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:36,372 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5100000000020373. input_tokens=29, output_tokens=0
18:40:36,374 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5030000000006112. input_tokens=28, output_tokens=0
18:40:36,377 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5050000000010186. input_tokens=27, output_tokens=0
18:40:36,379 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.514999999999418. input_tokens=28, output_tokens=0
18:40:36,404 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:36,406 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6650000000008731. input_tokens=31, output_tokens=0
18:40:36,560 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
18:40:36,562 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6980000000003201. input_tokens=30, output_tokens=0
18:40:36,575 datashaper.workflow.workflow INFO executing verb drop
18:40:36,582 datashaper.workflow.workflow INFO executing verb filter
18:40:36,593 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:40:36,741 graphrag.index.run INFO Running workflow: create_final_nodes...
18:40:36,746 graphrag.index.run INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:40:36,747 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
18:40:36,764 datashaper.workflow.workflow INFO executing verb layout_graph
18:40:36,814 datashaper.workflow.workflow INFO executing verb unpack_graph
18:40:36,833 datashaper.workflow.workflow INFO executing verb unpack_graph
18:40:36,853 datashaper.workflow.workflow INFO executing verb filter
18:40:36,871 datashaper.workflow.workflow INFO executing verb drop
18:40:36,878 datashaper.workflow.workflow INFO executing verb select
18:40:36,886 datashaper.workflow.workflow INFO executing verb rename
18:40:36,893 datashaper.workflow.workflow INFO executing verb convert
18:40:36,915 datashaper.workflow.workflow INFO executing verb join
18:40:36,926 datashaper.workflow.workflow INFO executing verb rename
18:40:36,927 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:40:37,46 graphrag.index.run INFO Running workflow: create_final_communities...
18:40:37,46 graphrag.index.run INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:40:37,47 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
18:40:37,66 datashaper.workflow.workflow INFO executing verb unpack_graph
18:40:37,85 datashaper.workflow.workflow INFO executing verb unpack_graph
18:40:37,104 datashaper.workflow.workflow INFO executing verb aggregate_override
18:40:37,113 datashaper.workflow.workflow INFO executing verb join
18:40:37,125 datashaper.workflow.workflow INFO executing verb join
18:40:37,137 datashaper.workflow.workflow INFO executing verb concat
18:40:37,145 datashaper.workflow.workflow INFO executing verb filter
18:40:37,176 datashaper.workflow.workflow INFO executing verb aggregate_override
18:40:37,187 datashaper.workflow.workflow INFO executing verb join
18:40:37,199 datashaper.workflow.workflow INFO executing verb filter
18:40:37,219 datashaper.workflow.workflow INFO executing verb fill
18:40:37,228 datashaper.workflow.workflow INFO executing verb merge
18:40:37,239 datashaper.workflow.workflow INFO executing verb copy
18:40:37,248 datashaper.workflow.workflow INFO executing verb select
18:40:37,249 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:40:37,365 graphrag.index.run INFO Running workflow: join_text_units_to_entity_ids...
18:40:37,366 graphrag.index.run INFO dependencies for join_text_units_to_entity_ids: ['create_final_entities']
18:40:37,366 graphrag.index.run INFO read table from storage: create_final_entities.parquet
18:40:37,393 datashaper.workflow.workflow INFO executing verb select
18:40:37,402 datashaper.workflow.workflow INFO executing verb unroll
18:40:37,412 datashaper.workflow.workflow INFO executing verb aggregate_override
18:40:37,415 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_entity_ids.parquet
18:40:37,529 graphrag.index.run INFO Running workflow: create_final_relationships...
18:40:37,529 graphrag.index.run INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
18:40:37,530 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
18:40:37,533 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
18:40:37,554 datashaper.workflow.workflow INFO executing verb unpack_graph
18:40:37,574 datashaper.workflow.workflow INFO executing verb filter
18:40:37,598 datashaper.workflow.workflow INFO executing verb rename
18:40:37,607 datashaper.workflow.workflow INFO executing verb filter
18:40:37,630 datashaper.workflow.workflow INFO executing verb drop
18:40:37,641 datashaper.workflow.workflow INFO executing verb compute_edge_combined_degree
18:40:37,654 datashaper.workflow.workflow INFO executing verb convert
18:40:37,675 datashaper.workflow.workflow INFO executing verb convert
18:40:37,676 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:40:37,805 graphrag.index.run INFO Running workflow: join_text_units_to_relationship_ids...
18:40:37,805 graphrag.index.run INFO dependencies for join_text_units_to_relationship_ids: ['create_final_relationships']
18:40:37,811 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
18:40:37,834 datashaper.workflow.workflow INFO executing verb select
18:40:37,845 datashaper.workflow.workflow INFO executing verb unroll
18:40:37,857 datashaper.workflow.workflow INFO executing verb aggregate_override
18:40:37,869 datashaper.workflow.workflow INFO executing verb select
18:40:37,870 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_relationship_ids.parquet
18:40:37,993 graphrag.index.run INFO Running workflow: create_final_community_reports...
18:40:37,993 graphrag.index.run INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_covariates', 'create_final_relationships']
18:40:37,993 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
18:40:37,997 graphrag.index.run INFO read table from storage: create_final_covariates.parquet
18:40:37,999 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
18:40:38,23 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
18:40:38,38 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
18:40:38,52 datashaper.workflow.workflow INFO executing verb prepare_community_reports_claims
18:40:38,65 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
18:40:38,80 datashaper.workflow.workflow INFO executing verb prepare_community_reports
18:40:38,80 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 302
18:40:38,105 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 302
18:40:38,158 datashaper.workflow.workflow INFO executing verb create_community_reports
18:41:00,193 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:00,194 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:00,195 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.02600000000166. input_tokens=2161, output_tokens=531
18:41:02,343 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:02,344 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:02,344 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.16599999999744. input_tokens=2726, output_tokens=547
18:41:02,958 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:02,958 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:02,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.791000000001077. input_tokens=2177, output_tokens=505
18:41:03,45 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:03,46 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:03,46 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.86699999999837. input_tokens=2219, output_tokens=510
18:41:04,567 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:04,567 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:04,568 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.402000000001863. input_tokens=2147, output_tokens=501
18:41:05,313 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:05,314 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:05,315 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 27.139999999999418. input_tokens=2192, output_tokens=578
18:41:07,668 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:07,669 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:07,669 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.492999999998574. input_tokens=2505, output_tokens=603
18:41:10,24 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:10,24 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:10,25 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 31.851999999998952. input_tokens=2088, output_tokens=537
18:41:10,843 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:10,843 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:10,844 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 32.661999999996624. input_tokens=2740, output_tokens=627
18:41:12,72 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:12,73 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:12,73 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 33.90200000000186. input_tokens=2296, output_tokens=595
18:41:12,276 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:12,276 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:12,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 34.11399999999776. input_tokens=4245, output_tokens=672
18:41:34,315 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:34,315 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:34,316 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.48700000000099. input_tokens=2078, output_tokens=504
18:41:37,876 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:37,877 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:37,877 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.038000000000466. input_tokens=2086, output_tokens=502
18:41:37,878 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:37,878 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:37,878 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.037000000000262. input_tokens=3597, output_tokens=565
18:41:38,286 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:38,286 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:38,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.449000000000524. input_tokens=2060, output_tokens=471
18:41:40,130 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:40,130 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:40,131 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 27.299999999999272. input_tokens=2474, output_tokens=615
18:41:46,510 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:46,510 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:46,511 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 33.67499999999927. input_tokens=4954, output_tokens=416
18:41:47,706 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:47,707 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:47,707 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 34.873999999999796. input_tokens=3038, output_tokens=747
18:41:52,419 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
18:41:52,420 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
18:41:52,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.57500000000073. input_tokens=9887, output_tokens=533
18:41:52,445 datashaper.workflow.workflow INFO executing verb window
18:41:52,447 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:41:52,581 graphrag.index.run INFO Running workflow: create_final_text_units...
18:41:52,581 graphrag.index.run INFO dependencies for create_final_text_units: ['join_text_units_to_entity_ids', 'join_text_units_to_covariate_ids', 'create_base_text_units', 'join_text_units_to_relationship_ids']
18:41:52,582 graphrag.index.run INFO read table from storage: join_text_units_to_entity_ids.parquet
18:41:52,584 graphrag.index.run INFO read table from storage: join_text_units_to_covariate_ids.parquet
18:41:52,586 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
18:41:52,588 graphrag.index.run INFO read table from storage: join_text_units_to_relationship_ids.parquet
18:41:52,613 datashaper.workflow.workflow INFO executing verb select
18:41:52,626 datashaper.workflow.workflow INFO executing verb rename
18:41:52,639 datashaper.workflow.workflow INFO executing verb join
18:41:52,655 datashaper.workflow.workflow INFO executing verb join
18:41:52,671 datashaper.workflow.workflow INFO executing verb join
18:41:52,686 datashaper.workflow.workflow INFO executing verb aggregate_override
18:41:52,702 datashaper.workflow.workflow INFO executing verb select
18:41:52,703 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:41:52,846 graphrag.index.run INFO Running workflow: create_base_documents...
18:41:52,847 graphrag.index.run INFO dependencies for create_base_documents: ['create_final_text_units']
18:41:52,847 graphrag.index.run INFO read table from storage: create_final_text_units.parquet
18:41:52,876 datashaper.workflow.workflow INFO executing verb unroll
18:41:52,890 datashaper.workflow.workflow INFO executing verb select
18:41:52,904 datashaper.workflow.workflow INFO executing verb rename
18:41:52,918 datashaper.workflow.workflow INFO executing verb join
18:41:52,935 datashaper.workflow.workflow INFO executing verb aggregate_override
18:41:52,951 datashaper.workflow.workflow INFO executing verb join
18:41:52,967 datashaper.workflow.workflow INFO executing verb rename
18:41:52,982 datashaper.workflow.workflow INFO executing verb convert
18:41:52,998 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
18:41:53,122 graphrag.index.run INFO Running workflow: create_final_documents...
18:41:53,123 graphrag.index.run INFO dependencies for create_final_documents: ['create_base_documents']
18:41:53,123 graphrag.index.run INFO read table from storage: create_base_documents.parquet
18:41:53,154 datashaper.workflow.workflow INFO executing verb rename
18:41:53,155 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
