21:36:35,771 graphrag.config.read_dotenv INFO Loading pipeline .env file
21:36:35,774 graphrag.index.cli INFO using default configuration: {
    "llm": {
        "api_key": "REDACTED, length 51",
        "type": "openai_chat",
        "model": "glm-4",
        "max_tokens": 2000,
        "temperature": 0.95,
        "top_p": 0.7,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:3000/v1",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "./",
    "reporting": {
        "type": "file",
        "base_dir": "inputs/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "inputs/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_embedding",
            "model": "embedding-2",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 1,
        "batch_max_tokens": 8000,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
21:36:35,774 graphrag.index.create_pipeline_config INFO skipping workflows 
21:36:35,777 graphrag.index.run INFO Running pipeline
21:36:35,777 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at inputs/artifacts
21:36:35,777 graphrag.index.input.load_input INFO loading input from root_dir=input
21:36:35,777 graphrag.index.input.load_input INFO using file storage for input
21:36:35,777 graphrag.index.storage.file_pipeline_storage INFO search input for files matching .*\.txt$
21:36:35,777 graphrag.index.input.text INFO found text files from input, found [('currentPrompt.txt', {})]
21:36:35,778 graphrag.index.input.text INFO Found 1 files, loading 1
21:36:35,779 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_final_covariates', 'create_summarized_entities', 'join_text_units_to_covariate_ids', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
21:36:35,779 graphrag.index.run INFO Final # of rows loaded: 1
21:36:35,870 graphrag.index.run INFO Running workflow: create_base_text_units...
21:36:35,870 graphrag.index.run INFO dependencies for create_base_text_units: []
21:36:35,872 datashaper.workflow.workflow INFO executing verb orderby
21:36:35,873 datashaper.workflow.workflow INFO executing verb zip
21:36:35,875 datashaper.workflow.workflow INFO executing verb aggregate_override
21:36:35,877 datashaper.workflow.workflow INFO executing verb chunk
21:36:35,968 datashaper.workflow.workflow INFO executing verb select
21:36:35,970 datashaper.workflow.workflow INFO executing verb unroll
21:36:35,973 datashaper.workflow.workflow INFO executing verb rename
21:36:35,975 datashaper.workflow.workflow INFO executing verb genid
21:36:35,978 datashaper.workflow.workflow INFO executing verb unzip
21:36:35,980 datashaper.workflow.workflow INFO executing verb copy
21:36:35,984 datashaper.workflow.workflow INFO executing verb filter
21:36:35,990 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:36:36,92 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
21:36:36,92 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
21:36:36,92 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
21:36:36,100 datashaper.workflow.workflow INFO executing verb entity_extract
21:36:36,102 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:3000/v1
21:36:36,106 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for glm-4: TPM=0, RPM=0
21:36:36,106 graphrag.index.llm.load_llm INFO create concurrency limiter for glm-4: 25
21:36:50,333 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:36:50,335 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.206999999994878. input_tokens=2935, output_tokens=184
21:37:24,643 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:37:24,644 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 48.53399999999965. input_tokens=2935, output_tokens=914
21:37:25,662 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:37:25,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.54400000000169. input_tokens=2936, output_tokens=793
21:37:30,276 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:37:30,278 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 54.14800000000105. input_tokens=2936, output_tokens=895
21:37:32,522 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:37:32,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 56.40899999999965. input_tokens=2938, output_tokens=1022
21:37:34,516 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:37:34,517 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 58.3949999999968. input_tokens=2936, output_tokens=1100
21:37:38,840 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:37:38,841 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 62.71499999999651. input_tokens=2936, output_tokens=1195
21:37:52,814 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:37:52,815 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 76.69900000000052. input_tokens=2936, output_tokens=1326
21:37:57,214 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:37:57,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 31.55000000000291. input_tokens=34, output_tokens=573
21:38:04,535 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:38:04,536 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 74.19999999999709. input_tokens=34, output_tokens=1366
21:38:06,323 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:38:06,324 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.677999999999884. input_tokens=34, output_tokens=550
21:38:10,216 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:38:10,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.6929999999993. input_tokens=34, output_tokens=481
21:38:11,742 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:38:11,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 95.62299999999959. input_tokens=2936, output_tokens=1225
21:38:15,651 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:38:15,652 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 45.37299999999959. input_tokens=34, output_tokens=791
21:38:18,90 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:38:18,92 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 101.96699999999691. input_tokens=2936, output_tokens=1995
21:38:32,17 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:38:32,18 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 53.175999999999476. input_tokens=34, output_tokens=895
21:38:43,390 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:38:43,391 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 127.26000000000204. input_tokens=2057, output_tokens=1866
21:38:57,214 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:38:57,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 64.39800000000105. input_tokens=34, output_tokens=978
21:39:13,695 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:39:13,696 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 99.17700000000332. input_tokens=34, output_tokens=2000
21:39:23,219 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:39:23,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 71.47600000000239. input_tokens=34, output_tokens=1254
21:39:52,940 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:39:52,941 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 94.84799999999814. input_tokens=34, output_tokens=2000
21:40:26,302 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:40:26,303 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 102.90999999999622. input_tokens=34, output_tokens=2000
21:40:26,311 datashaper.workflow.workflow INFO executing verb merge_graphs
21:40:26,323 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
21:40:26,420 graphrag.index.run INFO Running workflow: create_final_covariates...
21:40:26,420 graphrag.index.run INFO dependencies for create_final_covariates: ['create_base_text_units']
21:40:26,420 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
21:40:26,428 datashaper.workflow.workflow INFO executing verb extract_covariates
21:40:52,927 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:40:52,928 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.477999999995518. input_tokens=2314, output_tokens=453
21:40:59,783 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:40:59,784 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.33099999999831. input_tokens=1435, output_tokens=477
21:40:59,784 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:40:59,785 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.34700000000157. input_tokens=2315, output_tokens=607
21:41:14,836 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:41:14,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 48.39100000000326. input_tokens=2315, output_tokens=903
21:41:18,97 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:41:18,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 51.66399999999703. input_tokens=2316, output_tokens=823
21:41:20,471 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:41:20,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 54.03600000000006. input_tokens=2315, output_tokens=1124
21:41:24,392 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:41:24,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 57.95100000000093. input_tokens=2315, output_tokens=1182
21:41:24,771 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:41:24,772 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 58.34000000000378. input_tokens=2313, output_tokens=986
21:41:28,919 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:41:28,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 62.479999999995925. input_tokens=2315, output_tokens=1071
21:41:42,381 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:41:42,382 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.45300000000134. input_tokens=19, output_tokens=1078
21:41:49,959 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:41:49,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 50.17199999999866. input_tokens=19, output_tokens=1016
21:41:52,625 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:41:52,626 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 86.17399999999907. input_tokens=2314, output_tokens=917
21:41:56,103 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:41:56,104 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.00499999999738. input_tokens=19, output_tokens=722
21:41:58,694 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:41:58,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 92.25099999999657. input_tokens=2315, output_tokens=1952
21:42:15,463 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:42:15,464 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.542999999997846. input_tokens=19, output_tokens=930
21:42:17,815 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:42:17,816 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 57.34300000000076. input_tokens=19, output_tokens=1215
21:42:20,785 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:42:20,786 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.391999999999825. input_tokens=19, output_tokens=932
21:42:20,870 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:42:20,871 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 81.08499999999913. input_tokens=19, output_tokens=1518
21:42:46,485 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:42:46,486 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 81.71300000000338. input_tokens=19, output_tokens=1634
21:42:48,336 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:42:48,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 93.49899999999616. input_tokens=19, output_tokens=1880
21:43:09,526 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:09,527 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 70.83099999999831. input_tokens=19, output_tokens=1616
21:43:24,66 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:24,67 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 91.44000000000233. input_tokens=19, output_tokens=1526
21:43:24,74 datashaper.workflow.workflow INFO executing verb window
21:43:24,78 datashaper.workflow.workflow INFO executing verb genid
21:43:24,81 datashaper.workflow.workflow INFO executing verb convert
21:43:24,90 datashaper.workflow.workflow INFO executing verb rename
21:43:24,96 datashaper.workflow.workflow INFO executing verb select
21:43:24,97 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_covariates.parquet
21:43:24,207 graphrag.index.run INFO Running workflow: create_summarized_entities...
21:43:24,207 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
21:43:24,207 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
21:43:24,217 datashaper.workflow.workflow INFO executing verb summarize_descriptions
21:43:26,21 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:26,22 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7690000000002328. input_tokens=134, output_tokens=10
21:43:26,342 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:26,342 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.095000000001164. input_tokens=152, output_tokens=23
21:43:26,761 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:26,761 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.503000000004249. input_tokens=155, output_tokens=25
21:43:27,40 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:27,41 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:27,41 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:27,41 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7819999999992433. input_tokens=152, output_tokens=39
21:43:27,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.802000000003318. input_tokens=177, output_tokens=60
21:43:27,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.790999999997439. input_tokens=158, output_tokens=39
21:43:27,63 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:27,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8070000000006985. input_tokens=161, output_tokens=43
21:43:27,101 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:27,102 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.856999999996333. input_tokens=165, output_tokens=47
21:43:27,366 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:27,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1009999999951106. input_tokens=153, output_tokens=33
21:43:27,487 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:27,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.2150000000037835. input_tokens=173, output_tokens=41
21:43:27,756 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:27,756 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4800000000032014. input_tokens=172, output_tokens=36
21:43:27,804 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:27,804 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5499999999956344. input_tokens=152, output_tokens=55
21:43:27,813 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:27,813 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.550999999999476. input_tokens=172, output_tokens=52
21:43:27,898 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:27,899 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6350000000020373. input_tokens=158, output_tokens=47
21:43:28,104 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:28,105 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.834999999999127. input_tokens=171, output_tokens=52
21:43:28,146 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:28,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.8989999999976135. input_tokens=200, output_tokens=63
21:43:28,192 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:28,193 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.949000000000524. input_tokens=202, output_tokens=92
21:43:28,210 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:28,210 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.934000000001106. input_tokens=164, output_tokens=50
21:43:28,702 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:28,702 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.453000000001339. input_tokens=197, output_tokens=86
21:43:28,868 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:28,868 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8260000000009313. input_tokens=150, output_tokens=31
21:43:28,902 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:28,902 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.559000000001106. input_tokens=180, output_tokens=59
21:43:28,953 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:28,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.711999999999534. input_tokens=275, output_tokens=105
21:43:28,966 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:28,966 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.697999999996682. input_tokens=201, output_tokens=78
21:43:29,89 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:29,89 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7230000000054133. input_tokens=135, output_tokens=14
21:43:29,317 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:29,318 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.055000000000291. input_tokens=158, output_tokens=31
21:43:29,802 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:29,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.757000000005064. input_tokens=154, output_tokens=46
21:43:29,803 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:29,803 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:29,805 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.760999999998603. input_tokens=158, output_tokens=58
21:43:29,805 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7030000000013388. input_tokens=161, output_tokens=47
21:43:30,185 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:30,186 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1229999999995925. input_tokens=163, output_tokens=59
21:43:30,195 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:30,195 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2969999999986612. input_tokens=155, output_tokens=26
21:43:30,358 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:30,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.870999999999185. input_tokens=167, output_tokens=55
21:43:30,368 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:30,369 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2219999999942956. input_tokens=153, output_tokens=44
21:43:30,787 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:30,788 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.595000000001164. input_tokens=147, output_tokens=21
21:43:30,816 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:30,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.559000000001106. input_tokens=157, output_tokens=51
21:43:30,827 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:30,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.065000000002328. input_tokens=171, output_tokens=62
21:43:30,938 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:30,939 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.620999999999185. input_tokens=156, output_tokens=26
21:43:30,954 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:30,955 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.150000000001455. input_tokens=168, output_tokens=57
21:43:31,135 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:31,136 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.8989999999976135. input_tokens=298, output_tokens=129
21:43:31,328 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:31,329 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.521999999997206. input_tokens=156, output_tokens=19
21:43:31,390 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:31,391 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.194999999999709. input_tokens=156, output_tokens=15
21:43:31,410 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:31,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7079999999987194. input_tokens=171, output_tokens=57
21:43:31,473 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:31,473 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.19999999999709. input_tokens=168, output_tokens=66
21:43:31,704 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:31,705 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.8910000000032596. input_tokens=163, output_tokens=80
21:43:31,951 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:31,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7660000000032596. input_tokens=162, output_tokens=26
21:43:31,971 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:31,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.603000000002794. input_tokens=151, output_tokens=24
21:43:32,162 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:32,162 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.140999999995984. input_tokens=160, output_tokens=51
21:43:32,361 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:32,362 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.002999999996973. input_tokens=159, output_tokens=19
21:43:32,362 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:32,363 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4090000000069267. input_tokens=178, output_tokens=64
21:43:32,415 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:32,416 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.599000000001979. input_tokens=158, output_tokens=23
21:43:32,674 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:32,675 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.772000000004482. input_tokens=160, output_tokens=20
21:43:32,736 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:32,737 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7699999999967986. input_tokens=160, output_tokens=61
21:43:33,282 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:33,283 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4950000000026193. input_tokens=155, output_tokens=42
21:43:33,487 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:33,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1600000000034925. input_tokens=155, output_tokens=29
21:43:34,102 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:34,103 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1480000000010477. input_tokens=163, output_tokens=63
21:43:34,151 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:34,152 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.347000000001572. input_tokens=168, output_tokens=45
21:43:34,188 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:34,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.319999999999709. input_tokens=152, output_tokens=59
21:43:34,291 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:34,292 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.535999999992782. input_tokens=167, output_tokens=54
21:43:34,333 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:34,334 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.198000000003958. input_tokens=160, output_tokens=56
21:43:35,23 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:35,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.085000000006403. input_tokens=186, output_tokens=74
21:43:35,228 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:35,229 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.017999999996391. input_tokens=162, output_tokens=65
21:43:35,432 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:35,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.630000000004657. input_tokens=160, output_tokens=48
21:43:35,741 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:35,742 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.636999999995169. input_tokens=179, output_tokens=66
21:43:36,354 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:36,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.264999999999418. input_tokens=157, output_tokens=67
21:43:36,560 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:43:36,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.73399999999674. input_tokens=157, output_tokens=55
21:43:36,574 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
21:43:36,671 graphrag.index.run INFO Running workflow: join_text_units_to_covariate_ids...
21:43:36,671 graphrag.index.run INFO dependencies for join_text_units_to_covariate_ids: ['create_final_covariates']
21:43:36,671 graphrag.index.run INFO read table from storage: create_final_covariates.parquet
21:43:36,685 datashaper.workflow.workflow INFO executing verb select
21:43:36,692 datashaper.workflow.workflow INFO executing verb aggregate_override
21:43:36,694 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_covariate_ids.parquet
21:43:36,797 graphrag.index.run INFO Running workflow: create_base_entity_graph...
21:43:36,797 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
21:43:36,797 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
21:43:36,808 datashaper.workflow.workflow INFO executing verb cluster_graph
21:43:36,864 datashaper.workflow.workflow INFO executing verb select
21:43:36,865 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:43:36,969 graphrag.index.run INFO Running workflow: create_final_entities...
21:43:36,969 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:43:36,969 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
21:43:36,981 datashaper.workflow.workflow INFO executing verb unpack_graph
21:43:37,2 datashaper.workflow.workflow INFO executing verb rename
21:43:37,7 datashaper.workflow.workflow INFO executing verb select
21:43:37,12 datashaper.workflow.workflow INFO executing verb dedupe
21:43:37,19 datashaper.workflow.workflow INFO executing verb rename
21:43:37,24 datashaper.workflow.workflow INFO executing verb filter
21:43:37,39 datashaper.workflow.workflow INFO executing verb text_split
21:43:37,45 datashaper.workflow.workflow INFO executing verb drop
21:43:37,51 datashaper.workflow.workflow INFO executing verb merge
21:43:37,91 datashaper.workflow.workflow INFO executing verb text_embed
21:43:37,92 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:3000/v1
21:43:37,95 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for embedding-2: TPM=0, RPM=0
21:43:37,95 graphrag.index.llm.load_llm INFO create concurrency limiter for embedding-2: 25
21:43:37,108 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 266 inputs via 266 snippets using 266 batches. max_batch_size=1, max_tokens=8000
21:43:37,504 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:37,506 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3929999999963911. input_tokens=97, output_tokens=0
21:43:37,972 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:37,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8600000000005821. input_tokens=25, output_tokens=0
21:43:37,975 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:37,977 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8649999999979627. input_tokens=132, output_tokens=0
21:43:38,506 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,508 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3940000000002328. input_tokens=57, output_tokens=0
21:43:38,510 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,510 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,510 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,510 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,511 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,511 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,511 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,511 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,511 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,511 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,511 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,512 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,514 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4000000000014552. input_tokens=27, output_tokens=0
21:43:38,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.400999999998021. input_tokens=36, output_tokens=0
21:43:38,519 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4059999999954016. input_tokens=110, output_tokens=0
21:43:38,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.408000000003085. input_tokens=14, output_tokens=0
21:43:38,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4090000000069267. input_tokens=31, output_tokens=0
21:43:38,526 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4110000000000582. input_tokens=57, output_tokens=0
21:43:38,529 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4130000000004657. input_tokens=57, output_tokens=0
21:43:38,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4150000000008731. input_tokens=80, output_tokens=0
21:43:38,534 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4179999999978463. input_tokens=20, output_tokens=0
21:43:38,536 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4239999999990687. input_tokens=65, output_tokens=0
21:43:38,539 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.423000000002503. input_tokens=52, output_tokens=0
21:43:38,541 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4250000000029104. input_tokens=40, output_tokens=0
21:43:38,816 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,817 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,817 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,817 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,817 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7040000000051805. input_tokens=49, output_tokens=0
21:43:38,821 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7079999999987194. input_tokens=51, output_tokens=0
21:43:38,824 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7070000000021537. input_tokens=23, output_tokens=0
21:43:38,826 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7119999999995343. input_tokens=69, output_tokens=0
21:43:38,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.713000000003376. input_tokens=45, output_tokens=0
21:43:38,868 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:38,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8929999999963911. input_tokens=50, output_tokens=0
21:43:39,120 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,120 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,121 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,121 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,121 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,121 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.0100000000020373. input_tokens=89, output_tokens=0
21:43:39,126 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.0120000000024447. input_tokens=57, output_tokens=0
21:43:39,128 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.0130000000062864. input_tokens=32, output_tokens=0
21:43:39,130 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1550000000061118. input_tokens=24, output_tokens=0
21:43:39,132 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6259999999965657. input_tokens=23, output_tokens=0
21:43:39,135 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.0210000000006403. input_tokens=45, output_tokens=0
21:43:39,428 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,430 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8870000000024447. input_tokens=25, output_tokens=0
21:43:39,655 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,657 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1169999999983702. input_tokens=18, output_tokens=0
21:43:39,658 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,658 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,658 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,659 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,661 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1520000000018626. input_tokens=72, output_tokens=0
21:43:39,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1220000000030268. input_tokens=53, output_tokens=0
21:43:39,666 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.121000000006461. input_tokens=27, output_tokens=0
21:43:39,668 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.125. input_tokens=5, output_tokens=0
21:43:39,944 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,944 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,945 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,945 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:39,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1189999999987776. input_tokens=64, output_tokens=0
21:43:39,949 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.121000000006461. input_tokens=28, output_tokens=0
21:43:39,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1220000000030268. input_tokens=20, output_tokens=0
21:43:39,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.125. input_tokens=24, output_tokens=0
21:43:40,247 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,247 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,250 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1129999999975553. input_tokens=21, output_tokens=0
21:43:40,250 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,250 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,250 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,251 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,251 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,253 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1160000000018044. input_tokens=18, output_tokens=0
21:43:40,256 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1200000000026193. input_tokens=29, output_tokens=0
21:43:40,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1229999999995925. input_tokens=22, output_tokens=0
21:43:40,260 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8299999999944703. input_tokens=21, output_tokens=0
21:43:40,263 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.125. input_tokens=36, output_tokens=0
21:43:40,265 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1300000000046566. input_tokens=32, output_tokens=0
21:43:40,478 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,478 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8119999999980791. input_tokens=21, output_tokens=0
21:43:40,483 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8150000000023283. input_tokens=67, output_tokens=0
21:43:40,484 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,484 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,484 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8290000000051805. input_tokens=22, output_tokens=0
21:43:40,490 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.819999999999709. input_tokens=17, output_tokens=0
21:43:40,492 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8229999999966822. input_tokens=4, output_tokens=0
21:43:40,707 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,707 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,707 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,709 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7549999999973807. input_tokens=25, output_tokens=0
21:43:40,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7569999999977881. input_tokens=19, output_tokens=0
21:43:40,714 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7589999999981956. input_tokens=36, output_tokens=0
21:43:40,781 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,781 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,783 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8300000000017462. input_tokens=20, output_tokens=0
21:43:40,784 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,786 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5200000000040745. input_tokens=21, output_tokens=0
21:43:40,789 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5230000000010477. input_tokens=22, output_tokens=0
21:43:40,797 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,797 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,799 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.254000000000815. input_tokens=24, output_tokens=0
21:43:40,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.9720000000015716. input_tokens=32, output_tokens=0
21:43:40,846 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,846 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,848 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.3040000000037253. input_tokens=33, output_tokens=0
21:43:40,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.9830000000001746. input_tokens=4, output_tokens=0
21:43:40,854 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,855 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,855 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,859 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.31600000000617. input_tokens=44, output_tokens=0
21:43:40,861 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.3150000000023283. input_tokens=20, output_tokens=0
21:43:40,864 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.3179999999993015. input_tokens=26, output_tokens=0
21:43:40,930 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,931 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,931 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:40,935 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6680000000051223. input_tokens=64, output_tokens=0
21:43:40,937 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6689999999944121. input_tokens=54, output_tokens=0
21:43:40,939 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6739999999990687. input_tokens=25, output_tokens=0
21:43:41,0 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:41,1 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:41,3 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7520000000004075. input_tokens=31, output_tokens=0
21:43:41,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7379999999975553. input_tokens=22, output_tokens=0
21:43:41,17 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:41,18 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:41,20 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.4760000000023865. input_tokens=54, output_tokens=0
21:43:41,22 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.4789999999993597. input_tokens=25, output_tokens=0
21:43:41,167 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:41,167 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:41,168 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:41,168 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:41,170 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.68399999999383. input_tokens=28, output_tokens=0
21:43:41,172 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.680000000000291. input_tokens=17, output_tokens=0
21:43:41,175 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6820000000006985. input_tokens=66, output_tokens=0
21:43:41,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.684000000001106. input_tokens=23, output_tokens=0
21:43:41,224 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:41,225 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:41,227 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7419999999983702. input_tokens=21, output_tokens=0
21:43:41,229 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5139999999955762. input_tokens=23, output_tokens=0
21:43:41,477 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:41,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7660000000032596. input_tokens=18, output_tokens=0
21:43:41,480 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:41,481 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:41,481 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:41,482 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:41,484 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6299999999973807. input_tokens=21, output_tokens=0
21:43:41,487 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.771999999997206. input_tokens=53, output_tokens=0
21:43:41,489 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6990000000005239. input_tokens=24, output_tokens=0
21:43:41,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7019999999974971. input_tokens=35, output_tokens=0
21:43:42,41 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,41 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,43 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0210000000006403. input_tokens=22, output_tokens=0
21:43:42,45 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1059999999997672. input_tokens=32, output_tokens=0
21:43:42,47 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,47 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,47 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,48 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,48 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,50 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8720000000030268. input_tokens=26, output_tokens=0
21:43:42,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0460000000020955. input_tokens=22, output_tokens=0
21:43:42,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2529999999969732. input_tokens=24, output_tokens=0
21:43:42,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8799999999973807. input_tokens=16, output_tokens=0
21:43:42,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0529999999998836. input_tokens=18, output_tokens=0
21:43:42,59 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,59 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.885999999998603. input_tokens=17, output_tokens=0
21:43:42,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8359999999956926. input_tokens=22, output_tokens=0
21:43:42,294 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.117999999994936. input_tokens=21, output_tokens=0
21:43:42,297 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,298 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,298 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,298 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,300 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4440000000031432. input_tokens=50, output_tokens=0
21:43:42,302 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8099999999976717. input_tokens=26, output_tokens=0
21:43:42,305 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0749999999970896. input_tokens=14, output_tokens=0
21:43:42,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8139999999984866. input_tokens=21, output_tokens=0
21:43:42,333 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,335 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8430000000007567. input_tokens=17, output_tokens=0
21:43:42,522 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.033000000003085. input_tokens=15, output_tokens=0
21:43:42,601 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,603 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.536999999996624. input_tokens=25, output_tokens=0
21:43:42,604 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,605 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7420000000056461. input_tokens=34, output_tokens=0
21:43:42,609 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8260000000009313. input_tokens=25, output_tokens=0
21:43:42,611 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,611 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,613 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.75. input_tokens=34, output_tokens=0
21:43:42,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8130000000019209. input_tokens=29, output_tokens=0
21:43:42,822 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,823 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,826 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7649999999994179. input_tokens=13, output_tokens=0
21:43:42,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.782999999995809. input_tokens=62, output_tokens=0
21:43:42,829 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,829 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,829 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,830 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,830 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,830 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,831 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7870000000038999. input_tokens=59, output_tokens=0
21:43:42,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7680000000036671. input_tokens=20, output_tokens=0
21:43:42,839 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7779999999984284. input_tokens=22, output_tokens=0
21:43:42,841 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7809999999954016. input_tokens=25, output_tokens=0
21:43:42,844 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.783000000003085. input_tokens=23, output_tokens=0
21:43:42,846 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7860000000000582. input_tokens=21, output_tokens=0
21:43:42,848 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5409999999974389. input_tokens=23, output_tokens=0
21:43:42,849 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,849 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,849 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,849 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,855 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.9140000000043074. input_tokens=9, output_tokens=0
21:43:42,857 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3759999999965657. input_tokens=25, output_tokens=0
21:43:42,860 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.9199999999982538. input_tokens=30, output_tokens=0
21:43:42,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8400000000037835. input_tokens=16, output_tokens=0
21:43:42,866 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.004000000000815. input_tokens=30, output_tokens=0
21:43:42,969 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:42,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6620000000038999. input_tokens=29, output_tokens=0
21:43:43,46 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,46 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,47 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,47 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,47 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,49 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7410000000018044. input_tokens=7, output_tokens=0
21:43:43,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7160000000003492. input_tokens=17, output_tokens=0
21:43:43,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7570000000050641. input_tokens=23, output_tokens=0
21:43:43,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7489999999961583. input_tokens=27, output_tokens=0
21:43:43,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5339999999996508. input_tokens=20, output_tokens=0
21:43:43,199 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.592000000004191. input_tokens=19, output_tokens=0
21:43:43,364 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,365 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,365 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7559999999939464. input_tokens=13, output_tokens=0
21:43:43,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7540000000008149. input_tokens=20, output_tokens=0
21:43:43,372 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7550000000046566. input_tokens=18, output_tokens=0
21:43:43,498 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,501 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6700000000055297. input_tokens=21, output_tokens=0
21:43:43,502 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,502 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,502 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,503 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,503 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,503 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,505 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6540000000022701. input_tokens=19, output_tokens=0
21:43:43,507 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6379999999990105. input_tokens=21, output_tokens=0
21:43:43,510 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6779999999998836. input_tokens=17, output_tokens=0
21:43:43,512 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6489999999976135. input_tokens=17, output_tokens=0
21:43:43,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.661999999996624. input_tokens=16, output_tokens=0
21:43:43,518 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.669000000001688. input_tokens=16, output_tokens=0
21:43:43,521 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,521 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6720000000059372. input_tokens=21, output_tokens=0
21:43:43,526 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.661999999996624. input_tokens=17, output_tokens=0
21:43:43,552 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,552 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,555 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6900000000023283. input_tokens=19, output_tokens=0
21:43:43,557 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4959999999991851. input_tokens=35, output_tokens=0
21:43:43,720 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.661999999996624. input_tokens=36, output_tokens=0
21:43:43,723 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,725 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6659999999974389. input_tokens=17, output_tokens=0
21:43:43,726 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,726 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,726 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,729 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7569999999977881. input_tokens=24, output_tokens=0
21:43:43,731 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5289999999949941. input_tokens=22, output_tokens=0
21:43:43,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6730000000025029. input_tokens=27, output_tokens=0
21:43:43,818 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9689999999973224. input_tokens=17, output_tokens=0
21:43:43,848 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,851 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.330999999998312. input_tokens=34, output_tokens=0
21:43:43,875 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,877 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3499999999985448. input_tokens=27, output_tokens=0
21:43:43,896 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,896 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,898 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0339999999996508. input_tokens=18, output_tokens=0
21:43:43,901 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0489999999990687. input_tokens=22, output_tokens=0
21:43:43,911 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,914 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0640000000057626. input_tokens=21, output_tokens=0
21:43:43,948 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,948 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,948 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:43,950 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4309999999968568. input_tokens=23, output_tokens=0
21:43:43,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.43400000000110595. input_tokens=23, output_tokens=0
21:43:43,955 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4360000000015134. input_tokens=25, output_tokens=0
21:43:43,998 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,1 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4800000000032014. input_tokens=21, output_tokens=0
21:43:44,48 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,51 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6779999999998836. input_tokens=26, output_tokens=0
21:43:44,76 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,77 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5519999999960419. input_tokens=22, output_tokens=0
21:43:44,81 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5229999999937718. input_tokens=21, output_tokens=0
21:43:44,81 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,84 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5269999999945867. input_tokens=23, output_tokens=0
21:43:44,132 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,135 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7629999999990105. input_tokens=23, output_tokens=0
21:43:44,140 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,141 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,143 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7700000000040745. input_tokens=28, output_tokens=0
21:43:44,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0859999999956926. input_tokens=21, output_tokens=0
21:43:44,177 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6589999999996508. input_tokens=16, output_tokens=0
21:43:44,242 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,243 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6410000000032596. input_tokens=16, output_tokens=0
21:43:44,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.521999999997206. input_tokens=9, output_tokens=0
21:43:44,248 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,248 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,252 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5169999999998254. input_tokens=10, output_tokens=0
21:43:44,254 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5190000000002328. input_tokens=10, output_tokens=0
21:43:44,304 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.42899999999644933. input_tokens=19, output_tokens=0
21:43:44,488 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,490 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.40600000000267755. input_tokens=16, output_tokens=0
21:43:44,579 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,581 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5. input_tokens=30, output_tokens=0
21:43:44,648 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,651 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.34399999999732245. input_tokens=21, output_tokens=0
21:43:44,701 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,704 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5239999999976135. input_tokens=26, output_tokens=0
21:43:44,800 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3110000000015134. input_tokens=20, output_tokens=0
21:43:44,854 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:44,856 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.120999999999185. input_tokens=9, output_tokens=0
21:43:44,999 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,1 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0869999999995343. input_tokens=15, output_tokens=0
21:43:45,25 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,28 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4459999999962747. input_tokens=21, output_tokens=0
21:43:45,40 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0869999999995343. input_tokens=16, output_tokens=0
21:43:45,371 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5540000000037253. input_tokens=13, output_tokens=0
21:43:45,400 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,402 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2679999999963911. input_tokens=25, output_tokens=0
21:43:45,550 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,552 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5970000000015716. input_tokens=63, output_tokens=0
21:43:45,612 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,613 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,613 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,613 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,615 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6600000000034925. input_tokens=89, output_tokens=0
21:43:45,618 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5360000000000582. input_tokens=24, output_tokens=0
21:43:45,620 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7629999999990105. input_tokens=22, output_tokens=0
21:43:45,623 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6209999999991851. input_tokens=20, output_tokens=0
21:43:45,658 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,659 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,661 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5089999999981956. input_tokens=25, output_tokens=0
21:43:45,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6120000000009895. input_tokens=22, output_tokens=0
21:43:45,672 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,674 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4239999999990687. input_tokens=24, output_tokens=0
21:43:45,812 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,812 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,813 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,816 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6640000000043074. input_tokens=31, output_tokens=0
21:43:45,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7760000000052969. input_tokens=16, output_tokens=0
21:43:45,821 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.794000000001688. input_tokens=6, output_tokens=0
21:43:45,833 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,833 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.033000000003085. input_tokens=11, output_tokens=0
21:43:45,838 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.1150000000052387. input_tokens=24, output_tokens=0
21:43:45,972 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:45,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.4729999999981374. input_tokens=26, output_tokens=0
21:43:46,57 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,60 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.805000000000291. input_tokens=18, output_tokens=0
21:43:46,60 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6889999999984866. input_tokens=14, output_tokens=0
21:43:46,243 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,243 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.9969999999957508. input_tokens=30, output_tokens=0
21:43:46,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.34599999999773. input_tokens=16, output_tokens=0
21:43:46,249 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,249 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,249 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,250 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,250 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,250 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,250 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,250 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,252 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6290000000008149. input_tokens=7, output_tokens=0
21:43:46,255 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6310000000012224. input_tokens=21, output_tokens=0
21:43:46,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.3580000000001746. input_tokens=13, output_tokens=0
21:43:46,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8559999999997672. input_tokens=9, output_tokens=0
21:43:46,261 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6390000000028522. input_tokens=5, output_tokens=0
21:43:46,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6399999999994179. input_tokens=8, output_tokens=0
21:43:46,266 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.602999999995518. input_tokens=30, output_tokens=0
21:43:46,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7160000000003492. input_tokens=14, output_tokens=0
21:43:46,273 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,274 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,274 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,274 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6129999999975553. input_tokens=26, output_tokens=0
21:43:46,279 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4570000000021537. input_tokens=31, output_tokens=0
21:43:46,281 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6069999999963329. input_tokens=26, output_tokens=0
21:43:46,284 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.46099999999569263. input_tokens=29, output_tokens=0
21:43:46,294 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,294 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,297 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4739999999947031. input_tokens=32, output_tokens=0
21:43:46,300 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.044000000001688. input_tokens=7, output_tokens=0
21:43:46,507 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,510 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4499999999970896. input_tokens=29, output_tokens=0
21:43:46,548 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,551 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7119999999995343. input_tokens=18, output_tokens=0
21:43:46,971 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,971 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7030000000013388. input_tokens=25, output_tokens=0
21:43:46,976 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.705999999998312. input_tokens=25, output_tokens=0
21:43:46,977 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,977 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,977 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,978 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,978 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,978 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,980 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1419999999998254. input_tokens=24, output_tokens=0
21:43:46,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.27900000000227. input_tokens=22, output_tokens=0
21:43:46,985 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.1339999999981956. input_tokens=10, output_tokens=0
21:43:46,987 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0120000000024447. input_tokens=26, output_tokens=0
21:43:46,989 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.338999999999942. input_tokens=18, output_tokens=0
21:43:46,992 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9289999999964493. input_tokens=21, output_tokens=0
21:43:46,995 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,996 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,996 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,996 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:46,999 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.489000000001397. input_tokens=23, output_tokens=0
21:43:47,2 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7319999999963329. input_tokens=21, output_tokens=0
21:43:47,4 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7340000000040163. input_tokens=18, output_tokens=0
21:43:47,7 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7210000000050059. input_tokens=18, output_tokens=0
21:43:47,190 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,190 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,193 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9209999999948195. input_tokens=31, output_tokens=0
21:43:47,195 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.922999999995227. input_tokens=12, output_tokens=0
21:43:47,196 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,196 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,199 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9130000000004657. input_tokens=29, output_tokens=0
21:43:47,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6500000000014552. input_tokens=21, output_tokens=0
21:43:47,241 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,241 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,243 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.940999999998894. input_tokens=17, output_tokens=0
21:43:47,244 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,244 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9460000000035507. input_tokens=21, output_tokens=0
21:43:47,249 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9760000000023865. input_tokens=34, output_tokens=0
21:43:47,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.966999999996915. input_tokens=28, output_tokens=0
21:43:47,252 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9850000000005821. input_tokens=30, output_tokens=0
21:43:47,430 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4250000000029104. input_tokens=12, output_tokens=0
21:43:47,440 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
21:43:47,441 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['DISNEY<("ENTITY":']}
21:43:47,569 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.32700000000477303. input_tokens=17, output_tokens=0
21:43:47,581 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,583 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3349999999991269. input_tokens=26, output_tokens=0
21:43:47,595 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,598 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.341999999996915. input_tokens=18, output_tokens=0
21:43:47,656 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4059999999954016. input_tokens=15, output_tokens=0
21:43:47,686 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.43599999999423744. input_tokens=15, output_tokens=0
21:43:47,708 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.717000000004191. input_tokens=5, output_tokens=0
21:43:47,792 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,795 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5409999999974389. input_tokens=17, output_tokens=0
21:43:47,859 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:47,861 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6659999999974389. input_tokens=15, output_tokens=0
21:43:48,33 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,35 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.46300000000337604. input_tokens=21, output_tokens=0
21:43:48,43 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,46 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3570000000036089. input_tokens=21, output_tokens=0
21:43:48,106 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,109 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1010000000023865. input_tokens=18, output_tokens=0
21:43:48,121 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.114000000001397. input_tokens=17, output_tokens=0
21:43:48,124 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,126 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.41400000000430737. input_tokens=20, output_tokens=0
21:43:48,308 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,308 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,310 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3170000000027358. input_tokens=6, output_tokens=0
21:43:48,313 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3179999999993015. input_tokens=16, output_tokens=0
21:43:48,313 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,314 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,314 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,314 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,314 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,316 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.0679999999993015. input_tokens=24, output_tokens=0
21:43:48,319 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7350000000005821. input_tokens=18, output_tokens=0
21:43:48,321 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3260000000009313. input_tokens=14, output_tokens=0
21:43:48,323 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.0389999999970314. input_tokens=23, output_tokens=0
21:43:48,326 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3320000000021537. input_tokens=3, output_tokens=0
21:43:48,342 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,346 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.338000000003376. input_tokens=15, output_tokens=0
21:43:48,456 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,457 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,457 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,459 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6640000000043074. input_tokens=19, output_tokens=0
21:43:48,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.485000000000582. input_tokens=26, output_tokens=0
21:43:48,464 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8649999999979627. input_tokens=19, output_tokens=0
21:43:48,473 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,476 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4400000000023283. input_tokens=19, output_tokens=0
21:43:48,734 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,737 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0780000000013388. input_tokens=30, output_tokens=0
21:43:48,779 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,782 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7889999999970314. input_tokens=20, output_tokens=0
21:43:48,914 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,919 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.716999999996915. input_tokens=14, output_tokens=0
21:43:48,954 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,955 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,955 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:48,957 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.09599999999773. input_tokens=17, output_tokens=0
21:43:48,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7639999999955762. input_tokens=15, output_tokens=0
21:43:48,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.760999999998603. input_tokens=15, output_tokens=0
21:43:49,2 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:49,4 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5710000000035507. input_tokens=19, output_tokens=0
21:43:49,196 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:49,199 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 1 retries took 0.32299999999668216. input_tokens=6, output_tokens=0
21:43:49,462 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
21:43:49,464 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.489000000001397. input_tokens=23, output_tokens=0
21:43:49,476 datashaper.workflow.workflow INFO executing verb drop
21:43:49,483 datashaper.workflow.workflow INFO executing verb filter
21:43:49,493 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:43:49,630 graphrag.index.run INFO Running workflow: create_final_nodes...
21:43:49,630 graphrag.index.run INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:43:49,631 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
21:43:49,646 datashaper.workflow.workflow INFO executing verb layout_graph
21:43:49,711 datashaper.workflow.workflow INFO executing verb unpack_graph
21:43:49,735 datashaper.workflow.workflow INFO executing verb unpack_graph
21:43:49,759 datashaper.workflow.workflow INFO executing verb drop
21:43:49,766 datashaper.workflow.workflow INFO executing verb filter
21:43:49,785 datashaper.workflow.workflow INFO executing verb select
21:43:49,792 datashaper.workflow.workflow INFO executing verb rename
21:43:49,798 datashaper.workflow.workflow INFO executing verb convert
21:43:49,820 datashaper.workflow.workflow INFO executing verb join
21:43:49,830 datashaper.workflow.workflow INFO executing verb rename
21:43:49,832 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:43:49,945 graphrag.index.run INFO Running workflow: create_final_communities...
21:43:49,945 graphrag.index.run INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:43:49,946 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
21:43:49,963 datashaper.workflow.workflow INFO executing verb unpack_graph
21:43:49,987 datashaper.workflow.workflow INFO executing verb unpack_graph
21:43:50,11 datashaper.workflow.workflow INFO executing verb aggregate_override
21:43:50,20 datashaper.workflow.workflow INFO executing verb join
21:43:50,32 datashaper.workflow.workflow INFO executing verb join
21:43:50,44 datashaper.workflow.workflow INFO executing verb concat
21:43:50,53 datashaper.workflow.workflow INFO executing verb filter
21:43:50,105 datashaper.workflow.workflow INFO executing verb aggregate_override
21:43:50,116 datashaper.workflow.workflow INFO executing verb join
21:43:50,127 datashaper.workflow.workflow INFO executing verb filter
21:43:50,146 datashaper.workflow.workflow INFO executing verb fill
21:43:50,155 datashaper.workflow.workflow INFO executing verb merge
21:43:50,167 datashaper.workflow.workflow INFO executing verb copy
21:43:50,176 datashaper.workflow.workflow INFO executing verb select
21:43:50,177 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:43:50,310 graphrag.index.run INFO Running workflow: join_text_units_to_entity_ids...
21:43:50,310 graphrag.index.run INFO dependencies for join_text_units_to_entity_ids: ['create_final_entities']
21:43:50,310 graphrag.index.run INFO read table from storage: create_final_entities.parquet
21:43:50,337 datashaper.workflow.workflow INFO executing verb select
21:43:50,346 datashaper.workflow.workflow INFO executing verb unroll
21:43:50,357 datashaper.workflow.workflow INFO executing verb aggregate_override
21:43:50,359 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_entity_ids.parquet
21:43:50,470 graphrag.index.run INFO Running workflow: create_final_relationships...
21:43:50,470 graphrag.index.run INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
21:43:50,471 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
21:43:50,474 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
21:43:50,497 datashaper.workflow.workflow INFO executing verb unpack_graph
21:43:50,524 datashaper.workflow.workflow INFO executing verb filter
21:43:50,549 datashaper.workflow.workflow INFO executing verb rename
21:43:50,560 datashaper.workflow.workflow INFO executing verb filter
21:43:50,587 datashaper.workflow.workflow INFO executing verb drop
21:43:50,599 datashaper.workflow.workflow INFO executing verb compute_edge_combined_degree
21:43:50,613 datashaper.workflow.workflow INFO executing verb convert
21:43:50,634 datashaper.workflow.workflow INFO executing verb convert
21:43:50,635 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:43:50,753 graphrag.index.run INFO Running workflow: join_text_units_to_relationship_ids...
21:43:50,753 graphrag.index.run INFO dependencies for join_text_units_to_relationship_ids: ['create_final_relationships']
21:43:50,753 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
21:43:50,778 datashaper.workflow.workflow INFO executing verb select
21:43:50,790 datashaper.workflow.workflow INFO executing verb unroll
21:43:50,802 datashaper.workflow.workflow INFO executing verb aggregate_override
21:43:50,815 datashaper.workflow.workflow INFO executing verb select
21:43:50,816 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_relationship_ids.parquet
21:43:50,933 graphrag.index.run INFO Running workflow: create_final_community_reports...
21:43:50,933 graphrag.index.run INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_covariates', 'create_final_relationships']
21:43:50,934 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
21:43:50,937 graphrag.index.run INFO read table from storage: create_final_covariates.parquet
21:43:50,940 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
21:43:50,964 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
21:43:50,978 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
21:43:50,993 datashaper.workflow.workflow INFO executing verb prepare_community_reports_claims
21:43:51,6 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
21:43:51,20 datashaper.workflow.workflow INFO executing verb prepare_community_reports
21:43:51,21 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 266
21:43:51,34 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 266
21:43:51,81 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 266
21:43:51,152 datashaper.workflow.workflow INFO executing verb create_community_reports
21:44:14,862 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:44:14,863 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:44:14,863 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.698000000003958. input_tokens=2130, output_tokens=516
21:44:20,230 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:44:20,231 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:44:20,231 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.068999999995867. input_tokens=2265, output_tokens=611
21:44:28,375 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:44:28,375 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:44:28,376 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.2190000000046. input_tokens=3229, output_tokens=696
21:44:56,535 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:44:56,536 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:44:56,536 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 27.622999999999593. input_tokens=2328, output_tokens=555
21:44:58,102 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:44:58,103 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:44:58,103 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.228000000002794. input_tokens=2187, output_tokens=489
21:44:59,307 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:44:59,308 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:44:59,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 30.41399999999703. input_tokens=2641, output_tokens=572
21:45:01,245 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:01,246 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:01,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 32.362999999997555. input_tokens=2530, output_tokens=620
21:45:02,577 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:02,577 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:02,578 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 33.675999999999476. input_tokens=2027, output_tokens=471
21:45:03,409 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:03,409 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:03,409 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 34.52300000000105. input_tokens=2275, output_tokens=555
21:45:04,11 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:04,12 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:04,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.11999999999534. input_tokens=2665, output_tokens=659
21:45:04,63 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:04,64 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:04,64 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.18499999999767. input_tokens=2271, output_tokens=567
21:45:04,359 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:04,359 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:04,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.44499999999971. input_tokens=2167, output_tokens=591
21:45:04,522 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:04,523 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:04,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.629000000000815. input_tokens=2112, output_tokens=575
21:45:04,523 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:04,524 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:04,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.63300000000163. input_tokens=2126, output_tokens=585
21:45:04,832 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:04,832 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:04,833 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.94799999999668. input_tokens=2803, output_tokens=705
21:45:05,653 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:05,654 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:05,654 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.75. input_tokens=2334, output_tokens=577
21:45:05,858 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:05,858 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:05,859 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.986000000004424. input_tokens=2150, output_tokens=634
21:45:05,891 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:05,892 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:05,892 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.98199999999633. input_tokens=2167, output_tokens=563
21:45:06,159 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:06,159 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:06,160 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.254000000000815. input_tokens=2296, output_tokens=666
21:45:07,493 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:07,493 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:07,494 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:07,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 38.58299999999872. input_tokens=2305, output_tokens=601
21:45:07,494 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:07,495 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 38.586999999999534. input_tokens=2486, output_tokens=715
21:45:08,516 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:08,517 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:08,517 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.63999999999942. input_tokens=2084, output_tokens=664
21:45:12,311 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:12,311 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:12,312 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 43.43099999999686. input_tokens=3598, output_tokens=737
21:45:14,563 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:14,564 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:14,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 45.674999999995634. input_tokens=2515, output_tokens=664
21:45:15,490 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:15,490 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:15,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 46.59399999999732. input_tokens=3577, output_tokens=723
21:45:17,16 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:17,17 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:17,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 48.09799999999814. input_tokens=9855, output_tokens=598
21:45:39,447 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:39,447 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:39,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.395000000004075. input_tokens=2257, output_tokens=571
21:45:43,743 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:43,744 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:43,744 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.667999999997846. input_tokens=2117, output_tokens=532
21:45:45,181 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:45,182 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:45,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 28.127000000000407. input_tokens=2127, output_tokens=439
21:45:46,817 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:46,817 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:46,818 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.743999999998778. input_tokens=2042, output_tokens=532
21:45:47,423 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:47,424 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:47,424 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 30.36699999999837. input_tokens=2066, output_tokens=562
21:45:47,556 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:47,556 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:47,557 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 30.49699999999575. input_tokens=2037, output_tokens=524
21:45:48,559 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:48,559 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:48,560 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 31.50900000000547. input_tokens=5092, output_tokens=523
21:45:48,575 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:48,575 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:48,576 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 31.516999999999825. input_tokens=2218, output_tokens=498
21:45:52,201 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:52,202 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:52,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.137000000002445. input_tokens=2680, output_tokens=637
21:45:52,478 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:52,478 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:52,479 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.41599999999744. input_tokens=2103, output_tokens=610
21:45:52,965 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:52,965 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:52,966 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.89600000000064. input_tokens=2808, output_tokens=620
21:45:53,91 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:53,91 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:53,92 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.01400000000285. input_tokens=2501, output_tokens=568
21:45:56,338 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:56,338 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:56,339 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.27100000000064. input_tokens=5259, output_tokens=538
21:45:58,181 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:45:58,182 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:45:58,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.10899999999674. input_tokens=3080, output_tokens=639
21:46:21,122 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
21:46:21,122 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:46:21,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 64.04200000000128. input_tokens=3365, output_tokens=1083
21:46:21,148 datashaper.workflow.workflow INFO executing verb window
21:46:21,150 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:46:21,302 graphrag.index.run INFO Running workflow: create_final_text_units...
21:46:21,303 graphrag.index.run INFO dependencies for create_final_text_units: ['join_text_units_to_entity_ids', 'join_text_units_to_covariate_ids', 'join_text_units_to_relationship_ids', 'create_base_text_units']
21:46:21,303 graphrag.index.run INFO read table from storage: join_text_units_to_entity_ids.parquet
21:46:21,305 graphrag.index.run INFO read table from storage: join_text_units_to_covariate_ids.parquet
21:46:21,306 graphrag.index.run INFO read table from storage: join_text_units_to_relationship_ids.parquet
21:46:21,308 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
21:46:21,334 datashaper.workflow.workflow INFO executing verb select
21:46:21,347 datashaper.workflow.workflow INFO executing verb rename
21:46:21,360 datashaper.workflow.workflow INFO executing verb join
21:46:21,375 datashaper.workflow.workflow INFO executing verb join
21:46:21,393 datashaper.workflow.workflow INFO executing verb join
21:46:21,408 datashaper.workflow.workflow INFO executing verb aggregate_override
21:46:21,423 datashaper.workflow.workflow INFO executing verb select
21:46:21,424 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:46:21,550 graphrag.index.run INFO Running workflow: create_base_documents...
21:46:21,550 graphrag.index.run INFO dependencies for create_base_documents: ['create_final_text_units']
21:46:21,550 graphrag.index.run INFO read table from storage: create_final_text_units.parquet
21:46:21,580 datashaper.workflow.workflow INFO executing verb unroll
21:46:21,595 datashaper.workflow.workflow INFO executing verb select
21:46:21,608 datashaper.workflow.workflow INFO executing verb rename
21:46:21,622 datashaper.workflow.workflow INFO executing verb join
21:46:21,638 datashaper.workflow.workflow INFO executing verb aggregate_override
21:46:21,654 datashaper.workflow.workflow INFO executing verb join
21:46:21,671 datashaper.workflow.workflow INFO executing verb rename
21:46:21,689 datashaper.workflow.workflow INFO executing verb convert
21:46:21,705 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
21:46:21,827 graphrag.index.run INFO Running workflow: create_final_documents...
21:46:21,827 graphrag.index.run INFO dependencies for create_final_documents: ['create_base_documents']
21:46:21,827 graphrag.index.run INFO read table from storage: create_base_documents.parquet
21:46:21,858 datashaper.workflow.workflow INFO executing verb rename
21:46:21,859 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
