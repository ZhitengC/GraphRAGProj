22:34:16,996 graphrag.config.read_dotenv INFO Loading pipeline .env file
22:34:16,998 graphrag.index.cli INFO using default configuration: {
    "llm": {
        "api_key": "REDACTED, length 51",
        "type": "openai_chat",
        "model": "glm-4",
        "max_tokens": 2000,
        "temperature": 0.95,
        "top_p": 0.7,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:3000/v1",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "./",
    "reporting": {
        "type": "file",
        "base_dir": "inputs/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "inputs/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_embedding",
            "model": "embedding-2",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 1,
        "batch_max_tokens": 8000,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:34:16,999 graphrag.index.create_pipeline_config INFO skipping workflows 
22:34:17,1 graphrag.index.run INFO Running pipeline
22:34:17,1 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at inputs/artifacts
22:34:17,1 graphrag.index.input.load_input INFO loading input from root_dir=input
22:34:17,1 graphrag.index.input.load_input INFO using file storage for input
22:34:17,1 graphrag.index.storage.file_pipeline_storage INFO search input for files matching .*\.txt$
22:34:17,1 graphrag.index.input.text INFO found text files from input, found [('currentPrompt.txt', {})]
22:34:17,3 graphrag.index.input.text INFO Found 1 files, loading 1
22:34:17,4 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_final_covariates', 'create_summarized_entities', 'join_text_units_to_covariate_ids', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
22:34:17,4 graphrag.index.run INFO Final # of rows loaded: 1
22:34:17,88 graphrag.index.run INFO Running workflow: create_base_text_units...
22:34:17,88 graphrag.index.run INFO dependencies for create_base_text_units: []
22:34:17,91 datashaper.workflow.workflow INFO executing verb orderby
22:34:17,92 datashaper.workflow.workflow INFO executing verb zip
22:34:17,94 datashaper.workflow.workflow INFO executing verb aggregate_override
22:34:17,96 datashaper.workflow.workflow INFO executing verb chunk
22:34:17,185 datashaper.workflow.workflow INFO executing verb select
22:34:17,187 datashaper.workflow.workflow INFO executing verb unroll
22:34:17,190 datashaper.workflow.workflow INFO executing verb rename
22:34:17,191 datashaper.workflow.workflow INFO executing verb genid
22:34:17,194 datashaper.workflow.workflow INFO executing verb unzip
22:34:17,196 datashaper.workflow.workflow INFO executing verb copy
22:34:17,198 datashaper.workflow.workflow INFO executing verb filter
22:34:17,204 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:34:17,300 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
22:34:17,300 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:34:17,301 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
22:34:17,309 datashaper.workflow.workflow INFO executing verb entity_extract
22:34:17,310 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:3000/v1
22:34:17,314 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for glm-4: TPM=0, RPM=0
22:34:17,314 graphrag.index.llm.load_llm INFO create concurrency limiter for glm-4: 25
22:34:56,803 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:34:56,805 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.47799999999552. input_tokens=2936, output_tokens=712
22:34:59,574 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:34:59,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 42.23800000000483. input_tokens=1848, output_tokens=572
22:35:01,617 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:35:01,618 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 44.28600000000006. input_tokens=2936, output_tokens=726
22:35:04,585 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:35:04,586 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.25200000000041. input_tokens=2936, output_tokens=857
22:35:07,759 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:35:07,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.434000000001106. input_tokens=2936, output_tokens=829
22:35:19,131 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:35:19,132 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.513000000006286. input_tokens=34, output_tokens=277
22:35:20,55 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:35:20,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 62.720000000001164. input_tokens=2936, output_tokens=1008
22:35:24,452 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:35:24,453 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.692000000002736. input_tokens=34, output_tokens=263
22:35:27,933 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:35:27,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 28.355999999999767. input_tokens=34, output_tokens=306
22:35:37,148 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:35:37,149 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.562000000005355. input_tokens=34, output_tokens=379
22:35:47,702 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:35:47,703 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 50.89600000000064. input_tokens=34, output_tokens=993
22:35:51,178 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:35:51,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 93.84800000000541. input_tokens=2936, output_tokens=1999
22:35:55,478 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:35:55,479 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 98.16100000000006. input_tokens=2935, output_tokens=1613
22:35:55,493 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:35:55,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 98.17199999999866. input_tokens=2936, output_tokens=1623
22:36:01,930 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:36:01,931 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 104.60700000000361. input_tokens=2936, output_tokens=1745
22:36:14,941 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:36:14,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 117.61300000000483. input_tokens=2935, output_tokens=1999
22:36:28,861 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:36:28,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 68.80500000000029. input_tokens=34, output_tokens=1195
22:36:55,285 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:36:55,286 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 59.80599999999686. input_tokens=34, output_tokens=848
22:37:00,709 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:37:00,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 58.77900000000227. input_tokens=34, output_tokens=963
22:37:18,833 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:37:18,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 83.33899999999994. input_tokens=34, output_tokens=1514
22:37:24,778 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:37:24,779 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 93.59900000000198. input_tokens=34, output_tokens=1999
22:37:51,500 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:37:51,501 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 96.55600000000413. input_tokens=34, output_tokens=1593
22:37:51,508 datashaper.workflow.workflow INFO executing verb merge_graphs
22:37:51,518 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:37:51,610 graphrag.index.run INFO Running workflow: create_final_covariates...
22:37:51,610 graphrag.index.run INFO dependencies for create_final_covariates: ['create_base_text_units']
22:37:51,610 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
22:37:51,621 datashaper.workflow.workflow INFO executing verb extract_covariates
22:38:07,479 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:38:07,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.836000000002969. input_tokens=1226, output_tokens=325
22:38:35,737 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:38:35,738 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 44.10100000000239. input_tokens=2315, output_tokens=634
22:38:41,778 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:38:41,779 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.13999999999942. input_tokens=2314, output_tokens=730
22:38:46,928 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:38:46,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.447000000000116. input_tokens=19, output_tokens=651
22:39:05,639 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:39:05,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 74.01399999999558. input_tokens=2313, output_tokens=1338
22:39:06,158 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:39:06,159 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 74.52300000000105. input_tokens=2314, output_tokens=1250
22:39:15,368 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:39:15,369 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.629000000000815. input_tokens=19, output_tokens=774
22:39:22,748 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:39:22,749 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 91.10800000000017. input_tokens=2315, output_tokens=1211
22:39:32,165 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:39:32,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 100.53800000000047. input_tokens=2315, output_tokens=1983
22:39:36,60 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:39:36,61 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 104.42799999999988. input_tokens=2315, output_tokens=1670
22:39:41,177 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:39:41,178 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 109.53600000000006. input_tokens=2314, output_tokens=1961
22:39:41,520 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:39:41,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 59.741000000001804. input_tokens=19, output_tokens=1194
22:39:44,459 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:39:44,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.398000000001048. input_tokens=19, output_tokens=85
22:39:51,312 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:39:51,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 119.68299999999726. input_tokens=2315, output_tokens=1997
22:39:58,790 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:39:58,792 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 127.16000000000349. input_tokens=2315, output_tokens=1930
22:40:20,495 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:40:20,496 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 57.745999999999185. input_tokens=19, output_tokens=903
22:40:24,78 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:40:24,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 77.91899999999441. input_tokens=19, output_tokens=1754
22:40:45,276 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:40:45,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 99.6359999999986. input_tokens=19, output_tokens=1999
22:40:48,662 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:40:48,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.87000000000262. input_tokens=19, output_tokens=1110
22:41:08,525 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:08,526 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 96.35800000000017. input_tokens=19, output_tokens=1992
22:41:24,910 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:24,911 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 93.59599999999773. input_tokens=19, output_tokens=1991
22:41:31,868 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:31,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 110.68899999999849. input_tokens=19, output_tokens=1985
22:41:31,876 datashaper.workflow.workflow INFO executing verb window
22:41:31,879 datashaper.workflow.workflow INFO executing verb genid
22:41:31,882 datashaper.workflow.workflow INFO executing verb convert
22:41:31,889 datashaper.workflow.workflow INFO executing verb rename
22:41:31,892 datashaper.workflow.workflow INFO executing verb select
22:41:31,893 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_covariates.parquet
22:41:31,997 graphrag.index.run INFO Running workflow: create_summarized_entities...
22:41:31,997 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:41:31,998 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
22:41:32,6 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:41:34,336 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:34,336 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.300999999999476. input_tokens=150, output_tokens=32
22:41:34,633 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:34,633 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.576999999997497. input_tokens=142, output_tokens=16
22:41:34,837 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:34,838 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7850000000034925. input_tokens=156, output_tokens=31
22:41:34,855 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:34,856 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8149999999950523. input_tokens=157, output_tokens=39
22:41:34,934 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:34,934 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.893000000003667. input_tokens=171, output_tokens=41
22:41:35,79 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:35,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0460000000020955. input_tokens=169, output_tokens=48
22:41:35,106 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:35,106 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0570000000006985. input_tokens=174, output_tokens=42
22:41:35,268 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:35,268 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.239000000001397. input_tokens=190, output_tokens=71
22:41:35,301 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:35,301 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.243000000002212. input_tokens=161, output_tokens=47
22:41:35,309 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:35,310 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.2600000000020373. input_tokens=162, output_tokens=44
22:41:35,500 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:35,500 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4459999999962747. input_tokens=159, output_tokens=31
22:41:35,861 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:35,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.8249999999970896. input_tokens=150, output_tokens=43
22:41:35,972 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:35,973 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.915999999997439. input_tokens=163, output_tokens=60
22:41:36,475 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:36,476 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6200000000026193. input_tokens=137, output_tokens=22
22:41:36,520 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:36,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6829999999972642. input_tokens=133, output_tokens=14
22:41:37,90 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:37,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4570000000021537. input_tokens=164, output_tokens=40
22:41:37,184 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:37,185 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.25. input_tokens=158, output_tokens=45
22:41:37,214 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:37,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.168000000005122. input_tokens=217, output_tokens=88
22:41:37,293 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:37,293 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.2409999999945285. input_tokens=204, output_tokens=101
22:41:37,358 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:37,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.253000000004249. input_tokens=154, output_tokens=36
22:41:37,414 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:37,414 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.359000000004016. input_tokens=181, output_tokens=49
22:41:37,606 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:37,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.563000000001921. input_tokens=169, output_tokens=69
22:41:37,985 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:37,985 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0120000000024447. input_tokens=156, output_tokens=32
22:41:37,996 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:37,996 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.132999999994354. input_tokens=158, output_tokens=44
22:41:38,20 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:38,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7189999999973224. input_tokens=167, output_tokens=51
22:41:38,97 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:38,97 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.017999999996391. input_tokens=173, output_tokens=64
22:41:38,208 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:38,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.709000000002561. input_tokens=165, output_tokens=57
22:41:38,213 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:38,213 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.1820000000006985. input_tokens=149, output_tokens=32
22:41:38,227 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:38,227 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9179999999978463. input_tokens=175, output_tokens=63
22:41:38,832 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:38,832 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.495000000002619. input_tokens=219, output_tokens=111
22:41:38,900 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:38,901 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8099999999976717. input_tokens=166, output_tokens=35
22:41:39,42 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:39,43 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.521999999997206. input_tokens=163, output_tokens=47
22:41:39,51 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:39,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.02100000000064. input_tokens=251, output_tokens=136
22:41:39,142 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:39,142 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.105000000003201. input_tokens=242, output_tokens=100
22:41:39,451 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:39,451 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.158000000003085. input_tokens=177, output_tokens=37
22:41:39,456 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:39,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0979999999981374. input_tokens=160, output_tokens=42
22:41:39,553 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:39,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.518000000003667. input_tokens=238, output_tokens=140
22:41:39,751 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:39,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.709000000002561. input_tokens=189, output_tokens=65
22:41:40,167 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:40,168 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5609999999942374. input_tokens=182, output_tokens=52
22:41:40,573 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:40,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.53399999999965. input_tokens=178, output_tokens=41
22:41:40,574 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:40,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.527000000001863. input_tokens=170, output_tokens=59
22:41:40,667 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:40,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.481999999996333. input_tokens=159, output_tokens=46
22:41:40,981 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:40,982 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.713000000003376. input_tokens=178, output_tokens=70
22:41:41,499 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:41,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.084999999999127. input_tokens=159, output_tokens=27
22:41:41,581 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:41,582 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.59599999999773. input_tokens=169, output_tokens=46
22:41:42,107 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:42,108 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.892999999996391. input_tokens=167, output_tokens=51
22:41:42,440 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:42,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.9649999999965075. input_tokens=162, output_tokens=49
22:41:44,777 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:41:44,778 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 12.731999999996333. input_tokens=232, output_tokens=103
22:41:44,787 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:41:44,878 graphrag.index.run INFO Running workflow: join_text_units_to_covariate_ids...
22:41:44,878 graphrag.index.run INFO dependencies for join_text_units_to_covariate_ids: ['create_final_covariates']
22:41:44,879 graphrag.index.run INFO read table from storage: create_final_covariates.parquet
22:41:44,891 datashaper.workflow.workflow INFO executing verb select
22:41:44,895 datashaper.workflow.workflow INFO executing verb aggregate_override
22:41:44,897 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_covariate_ids.parquet
22:41:44,995 graphrag.index.run INFO Running workflow: create_base_entity_graph...
22:41:44,995 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:41:44,995 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
22:41:45,6 datashaper.workflow.workflow INFO executing verb cluster_graph
22:41:45,37 datashaper.workflow.workflow INFO executing verb select
22:41:45,39 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:41:45,140 graphrag.index.run INFO Running workflow: create_final_entities...
22:41:45,140 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:41:45,140 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
22:41:45,152 datashaper.workflow.workflow INFO executing verb unpack_graph
22:41:45,165 datashaper.workflow.workflow INFO executing verb rename
22:41:45,170 datashaper.workflow.workflow INFO executing verb select
22:41:45,175 datashaper.workflow.workflow INFO executing verb dedupe
22:41:45,180 datashaper.workflow.workflow INFO executing verb rename
22:41:45,185 datashaper.workflow.workflow INFO executing verb filter
22:41:45,196 datashaper.workflow.workflow INFO executing verb text_split
22:41:45,203 datashaper.workflow.workflow INFO executing verb drop
22:41:45,208 datashaper.workflow.workflow INFO executing verb merge
22:41:45,230 datashaper.workflow.workflow INFO executing verb text_embed
22:41:45,230 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:3000/v1
22:41:45,234 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for embedding-2: TPM=0, RPM=0
22:41:45,234 graphrag.index.llm.load_llm INFO create concurrency limiter for embedding-2: 25
22:41:45,239 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 225 inputs via 225 snippets using 225 batches. max_batch_size=1, max_tokens=8000
22:41:45,608 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:45,612 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3690000000060536. input_tokens=40, output_tokens=0
22:41:45,676 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:45,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4330000000045402. input_tokens=29, output_tokens=0
22:41:45,703 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:45,703 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:45,705 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.46199999999953434. input_tokens=74, output_tokens=0
22:41:45,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4629999999961001. input_tokens=28, output_tokens=0
22:41:45,792 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:45,793 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:45,795 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5499999999956344. input_tokens=55, output_tokens=0
22:41:45,797 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5519999999960419. input_tokens=39, output_tokens=0
22:41:45,858 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:45,858 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:45,861 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6160000000018044. input_tokens=17, output_tokens=0
22:41:45,863 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6190000000060536. input_tokens=22, output_tokens=0
22:41:45,864 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:45,864 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:45,865 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:45,865 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:45,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6219999999957508. input_tokens=32, output_tokens=0
22:41:45,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6219999999957508. input_tokens=15, output_tokens=0
22:41:45,871 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.625. input_tokens=24, output_tokens=0
22:41:45,874 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6280000000042492. input_tokens=27, output_tokens=0
22:41:45,876 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:45,877 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:45,879 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6330000000016298. input_tokens=18, output_tokens=0
22:41:45,881 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.636000000005879. input_tokens=28, output_tokens=0
22:41:45,915 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:45,918 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6749999999956344. input_tokens=33, output_tokens=0
22:41:46,6 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,9 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7629999999990105. input_tokens=16, output_tokens=0
22:41:46,9 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,12 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3989999999976135. input_tokens=20, output_tokens=0
22:41:46,80 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,82 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3760000000038417. input_tokens=19, output_tokens=0
22:41:46,92 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,94 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4150000000008731. input_tokens=24, output_tokens=0
22:41:46,101 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,103 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.39500000000407454. input_tokens=44, output_tokens=0
22:41:46,131 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,133 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.885999999998603. input_tokens=7, output_tokens=0
22:41:46,133 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,136 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8909999999959837. input_tokens=142, output_tokens=0
22:41:46,151 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,153 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.35499999999592546. input_tokens=26, output_tokens=0
22:41:46,174 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3790000000008149. input_tokens=21, output_tokens=0
22:41:46,222 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.34900000000197906. input_tokens=52, output_tokens=0
22:41:46,283 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,283 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,284 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,286 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0389999999970314. input_tokens=16, output_tokens=0
22:41:46,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0420000000012806. input_tokens=146, output_tokens=0
22:41:46,291 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0460000000020955. input_tokens=37, output_tokens=0
22:41:46,325 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,327 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.081000000005588. input_tokens=33, output_tokens=0
22:41:46,346 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,346 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,346 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,348 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1019999999989523. input_tokens=23, output_tokens=0
22:41:46,350 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1050000000032014. input_tokens=31, output_tokens=0
22:41:46,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1059999999997672. input_tokens=16, output_tokens=0
22:41:46,379 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,381 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5060000000012224. input_tokens=50, output_tokens=0
22:41:46,423 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.34399999999732245. input_tokens=104, output_tokens=0
22:41:46,621 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,623 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7410000000018044. input_tokens=68, output_tokens=0
22:41:46,641 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,643 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7680000000036671. input_tokens=45, output_tokens=0
22:41:46,661 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,664 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8000000000029104. input_tokens=35, output_tokens=0
22:41:46,664 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5310000000026776. input_tokens=43, output_tokens=0
22:41:46,670 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,672 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7969999999986612. input_tokens=28, output_tokens=0
22:41:46,781 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,783 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4919999999983702. input_tokens=22, output_tokens=0
22:41:46,836 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,839 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.547999999995227. input_tokens=109, output_tokens=0
22:41:46,854 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,856 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.569999999999709. input_tokens=72, output_tokens=0
22:41:46,872 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,872 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,874 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8660000000018044. input_tokens=29, output_tokens=0
22:41:46,877 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5230000000010477. input_tokens=23, output_tokens=0
22:41:46,878 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:46,880 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8680000000022119. input_tokens=45, output_tokens=0
22:41:46,998 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:47,0 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8470000000015716. input_tokens=47, output_tokens=0
22:41:47,1 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:47,1 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:47,4 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6509999999980209. input_tokens=17, output_tokens=0
22:41:47,7 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0890000000072177. input_tokens=45, output_tokens=0
22:41:47,8 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:47,8 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:47,11 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3669999999983702. input_tokens=24, output_tokens=0
22:41:47,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3459999999977299. input_tokens=19, output_tokens=0
22:41:47,68 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:47,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4470000000001164. input_tokens=24, output_tokens=0
22:41:47,86 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:47,88 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4239999999990687. input_tokens=46, output_tokens=0
22:41:47,108 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:47,110 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.43800000000192085. input_tokens=49, output_tokens=0
22:41:47,291 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:47,293 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9660000000003492. input_tokens=23, output_tokens=0
22:41:47,440 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:47,442 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5860000000029686. input_tokens=28, output_tokens=0
22:41:47,470 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:47,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6339999999981956. input_tokens=24, output_tokens=0
22:41:47,503 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:47,505 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1189999999987776. input_tokens=29, output_tokens=0
22:41:47,602 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:47,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7269999999989523. input_tokens=30, output_tokens=0
22:41:47,760 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:47,762 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8829999999943539. input_tokens=34, output_tokens=0
22:41:48,285 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.184000000001106. input_tokens=52, output_tokens=0
22:41:48,329 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,329 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,330 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,330 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,330 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,330 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,330 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,330 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,331 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,333 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.330999999998312. input_tokens=25, output_tokens=0
22:41:48,335 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.157999999995809. input_tokens=49, output_tokens=0
22:41:48,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2490000000034343. input_tokens=25, output_tokens=0
22:41:48,340 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2690000000002328. input_tokens=28, output_tokens=0
22:41:48,342 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.048000000002503. input_tokens=37, output_tokens=0
22:41:48,344 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.1200000000026193. input_tokens=48, output_tokens=0
22:41:48,347 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3389999999999418. input_tokens=9, output_tokens=0
22:41:48,349 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.47099999999773. input_tokens=28, output_tokens=0
22:41:48,352 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5679999999993015. input_tokens=107, output_tokens=0
22:41:48,352 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,352 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3539999999993597. input_tokens=23, output_tokens=0
22:41:48,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.227000000006228. input_tokens=40, output_tokens=0
22:41:48,545 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,545 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,546 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,548 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1050000000032014. input_tokens=58, output_tokens=0
22:41:48,550 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.1960000000035507. input_tokens=94, output_tokens=0
22:41:48,550 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,553 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0800000000017462. input_tokens=21, output_tokens=0
22:41:48,555 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0500000000029104. input_tokens=22, output_tokens=0
22:41:48,572 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8110000000015134. input_tokens=26, output_tokens=0
22:41:48,633 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,633 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,634 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,636 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5249999999941792. input_tokens=35, output_tokens=0
22:41:48,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.032999999995809. input_tokens=21, output_tokens=0
22:41:48,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.213999999999942. input_tokens=65, output_tokens=0
22:41:48,704 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,707 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6929999999993015. input_tokens=34, output_tokens=0
22:41:48,842 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,844 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.830999999998312. input_tokens=35, output_tokens=0
22:41:48,914 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:48,917 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.0529999999998836. input_tokens=46, output_tokens=0
22:41:49,5 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,7 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.9130000000004657. input_tokens=47, output_tokens=0
22:41:49,7 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,10 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.127999999996973. input_tokens=46, output_tokens=0
22:41:49,54 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,54 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7030000000013388. input_tokens=56, output_tokens=0
22:41:49,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7710000000006403. input_tokens=30, output_tokens=0
22:41:49,264 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,266 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9110000000000582. input_tokens=23, output_tokens=0
22:41:49,267 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,267 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,268 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,268 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,268 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,269 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,269 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,269 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,273 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9159999999974389. input_tokens=36, output_tokens=0
22:41:49,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9139999999970314. input_tokens=22, output_tokens=0
22:41:49,278 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9249999999956344. input_tokens=26, output_tokens=0
22:41:49,280 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9270000000033178. input_tokens=64, output_tokens=0
22:41:49,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9279999999998836. input_tokens=49, output_tokens=0
22:41:49,285 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9320000000006985. input_tokens=25, output_tokens=0
22:41:49,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9250000000029104. input_tokens=18, output_tokens=0
22:41:49,289 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.934000000001106. input_tokens=9, output_tokens=0
22:41:49,310 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,312 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9569999999948777. input_tokens=113, output_tokens=0
22:41:49,479 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,479 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,480 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,480 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,482 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9310000000041327. input_tokens=27, output_tokens=0
22:41:49,484 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8430000000007567. input_tokens=20, output_tokens=0
22:41:49,487 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8459999999977299. input_tokens=31, output_tokens=0
22:41:49,489 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8479999999981374. input_tokens=20, output_tokens=0
22:41:49,520 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6779999999998836. input_tokens=32, output_tokens=0
22:41:49,633 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,635 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0799999999944703. input_tokens=37, output_tokens=0
22:41:49,636 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.088000000003376. input_tokens=26, output_tokens=0
22:41:49,640 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,642 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7260000000023865. input_tokens=29, output_tokens=0
22:41:49,693 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,693 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,694 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,696 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.40499999999883585. input_tokens=25, output_tokens=0
22:41:49,698 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6390000000028522. input_tokens=20, output_tokens=0
22:41:49,700 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.40899999999965075. input_tokens=19, output_tokens=0
22:41:49,850 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,850 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,850 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,851 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2779999999984284. input_tokens=23, output_tokens=0
22:41:49,855 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7949999999982538. input_tokens=49, output_tokens=0
22:41:49,857 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8499999999985448. input_tokens=26, output_tokens=0
22:41:49,860 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8490000000019791. input_tokens=27, output_tokens=0
22:41:49,909 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,910 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,912 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5990000000019791. input_tokens=18, output_tokens=0
22:41:49,912 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,913 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,913 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:49,915 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6230000000068685. input_tokens=16, output_tokens=0
22:41:49,918 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6269999999931315. input_tokens=16, output_tokens=0
22:41:49,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6279999999969732. input_tokens=16, output_tokens=0
22:41:49,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6310000000012224. input_tokens=19, output_tokens=0
22:41:50,53 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,55 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3479999999981374. input_tokens=27, output_tokens=0
22:41:50,122 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,123 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,123 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,125 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8349999999991269. input_tokens=24, output_tokens=0
22:41:50,127 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.838000000003376. input_tokens=25, output_tokens=0
22:41:50,130 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8630000000048312. input_tokens=22, output_tokens=0
22:41:50,239 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,241 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.38100000000122236. input_tokens=23, output_tokens=0
22:41:50,388 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,390 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8989999999976135. input_tokens=42, output_tokens=0
22:41:50,402 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,404 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8819999999977881. input_tokens=14, output_tokens=0
22:41:50,405 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,406 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,406 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,406 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,406 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,406 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,408 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7069999999948777. input_tokens=22, output_tokens=0
22:41:50,416 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9260000000067521. input_tokens=20, output_tokens=0
22:41:50,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9290000000037253. input_tokens=71, output_tokens=0
22:41:50,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7200000000011642. input_tokens=13, output_tokens=0
22:41:50,423 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7229999999981374. input_tokens=17, output_tokens=0
22:41:50,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9349999999976717. input_tokens=20, output_tokens=0
22:41:50,609 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,611 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6889999999984866. input_tokens=25, output_tokens=0
22:41:50,619 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,619 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,620 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,622 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6980000000039581. input_tokens=23, output_tokens=0
22:41:50,624 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7000000000043656. input_tokens=21, output_tokens=0
22:41:50,626 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7030000000013388. input_tokens=22, output_tokens=0
22:41:50,627 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,630 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5740000000005239. input_tokens=31, output_tokens=0
22:41:50,750 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:50,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.510999999998603. input_tokens=23, output_tokens=0
22:41:51,49 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,49 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,51 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6240000000034343. input_tokens=14, output_tokens=0
22:41:51,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9239999999990687. input_tokens=19, output_tokens=0
22:41:51,55 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,55 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,55 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6290000000008149. input_tokens=20, output_tokens=0
22:41:51,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6330000000016298. input_tokens=24, output_tokens=0
22:41:51,62 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.422999999995227. input_tokens=38, output_tokens=0
22:41:51,161 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,162 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,162 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,164 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.033000000003085. input_tokens=22, output_tokens=0
22:41:51,166 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7620000000024447. input_tokens=21, output_tokens=0
22:41:51,168 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3079999999972642. input_tokens=21, output_tokens=0
22:41:51,169 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,172 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0420000000012806. input_tokens=22, output_tokens=0
22:41:51,173 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,173 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,173 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,173 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,173 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3149999999950523. input_tokens=18, output_tokens=0
22:41:51,178 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3169999999954598. input_tokens=11, output_tokens=0
22:41:51,180 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.268000000003667. input_tokens=57, output_tokens=0
22:41:51,183 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7560000000012224. input_tokens=19, output_tokens=0
22:41:51,185 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7570000000050641. input_tokens=16, output_tokens=0
22:41:51,265 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,268 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8770000000004075. input_tokens=20, output_tokens=0
22:41:51,432 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,434 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0060000000012224. input_tokens=20, output_tokens=0
22:41:51,435 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,437 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6849999999976717. input_tokens=22, output_tokens=0
22:41:51,438 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,438 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,438 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,438 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,438 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8289999999979045. input_tokens=25, output_tokens=0
22:41:51,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8170000000027358. input_tokens=29, output_tokens=0
22:41:51,446 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8179999999993015. input_tokens=21, output_tokens=0
22:41:51,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.819999999999709. input_tokens=25, output_tokens=0
22:41:51,450 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.819999999999709. input_tokens=5, output_tokens=0
22:41:51,453 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,453 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,454 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,456 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8139999999984866. input_tokens=20, output_tokens=0
22:41:51,459 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.9029999999984284. input_tokens=31, output_tokens=0
22:41:51,461 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8250000000043656. input_tokens=75, output_tokens=0
22:41:51,477 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,479 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.41700000000128057. input_tokens=3, output_tokens=0
22:41:51,480 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,482 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4210000000020955. input_tokens=4, output_tokens=0
22:41:51,740 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6899999999950523. input_tokens=5, output_tokens=0
22:41:51,744 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,744 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,746 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6920000000027358. input_tokens=3, output_tokens=0
22:41:51,749 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6860000000015134. input_tokens=5, output_tokens=0
22:41:51,956 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,958 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7889999999970314. input_tokens=3, output_tokens=0
22:41:51,958 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,959 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,959 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,960 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,960 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,960 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:51,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6939999999958673. input_tokens=28, output_tokens=0
22:41:51,964 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7770000000018626. input_tokens=18, output_tokens=0
22:41:51,967 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7930000000051223. input_tokens=32, output_tokens=0
22:41:51,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7810000000026776. input_tokens=26, output_tokens=0
22:41:51,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7840000000069267. input_tokens=20, output_tokens=0
22:41:51,973 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8040000000037253. input_tokens=4, output_tokens=0
22:41:52,173 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,173 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,174 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,174 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,174 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,174 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7249999999985448. input_tokens=29, output_tokens=0
22:41:52,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7260000000023865. input_tokens=24, output_tokens=0
22:41:52,181 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7010000000009313. input_tokens=67, output_tokens=0
22:41:52,183 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7220000000015716. input_tokens=29, output_tokens=0
22:41:52,186 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.43699999999807915. input_tokens=21, output_tokens=0
22:41:52,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7249999999985448. input_tokens=72, output_tokens=0
22:41:52,217 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7849999999962165. input_tokens=26, output_tokens=0
22:41:52,255 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,255 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0710000000035507. input_tokens=28, output_tokens=0
22:41:52,260 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5100000000020373. input_tokens=18, output_tokens=0
22:41:52,261 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5200000000040745. input_tokens=43, output_tokens=0
22:41:52,390 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,390 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2070000000021537. input_tokens=31, output_tokens=0
22:41:52,395 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4210000000020955. input_tokens=29, output_tokens=0
22:41:52,403 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,405 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9539999999979045. input_tokens=25, output_tokens=0
22:41:52,468 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,468 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,469 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,469 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,471 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5120000000024447. input_tokens=19, output_tokens=0
22:41:52,473 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4970000000030268. input_tokens=25, output_tokens=0
22:41:52,476 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5. input_tokens=30, output_tokens=0
22:41:52,478 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5040000000008149. input_tokens=27, output_tokens=0
22:41:52,480 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,482 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0319999999992433. input_tokens=26, output_tokens=0
22:41:52,603 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,604 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,604 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,606 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6310000000012224. input_tokens=17, output_tokens=0
22:41:52,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.419000000001688. input_tokens=23, output_tokens=0
22:41:52,611 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4210000000020955. input_tokens=21, output_tokens=0
22:41:52,612 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,614 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.42599999999947613. input_tokens=26, output_tokens=0
22:41:52,685 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.49799999999959255. input_tokens=21, output_tokens=0
22:41:52,688 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.238999999994121. input_tokens=30, output_tokens=0
22:41:52,691 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,694 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5050000000046566. input_tokens=39, output_tokens=0
22:41:52,726 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,728 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.46899999999732245. input_tokens=19, output_tokens=0
22:41:52,820 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,820 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,822 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.42700000000331784. input_tokens=12, output_tokens=0
22:41:52,825 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6370000000024447. input_tokens=28, output_tokens=0
22:41:52,826 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4320000000006985. input_tokens=12, output_tokens=0
22:41:52,830 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,832 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.34900000000197906. input_tokens=15, output_tokens=0
22:41:52,846 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,848 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4429999999993015. input_tokens=11, output_tokens=0
22:41:52,900 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,901 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:52,903 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4639999999999418. input_tokens=24, output_tokens=0
22:41:52,905 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.42700000000331784. input_tokens=14, output_tokens=0
22:41:53,27 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5469999999986612. input_tokens=21, output_tokens=0
22:41:53,30 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,32 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8119999999980791. input_tokens=24, output_tokens=0
22:41:53,32 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,33 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,35 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.555000000000291. input_tokens=11, output_tokens=0
22:41:53,37 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5579999999972642. input_tokens=9, output_tokens=0
22:41:53,38 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5610000000015134. input_tokens=10, output_tokens=0
22:41:53,40 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,41 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,43 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4309999999968568. input_tokens=28, output_tokens=0
22:41:53,45 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.43000000000029104. input_tokens=27, output_tokens=0
22:41:53,113 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,115 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5040000000008149. input_tokens=34, output_tokens=0
22:41:53,116 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,116 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,118 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5069999999977881. input_tokens=9, output_tokens=0
22:41:53,120 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8569999999963329. input_tokens=11, output_tokens=0
22:41:53,155 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3319999999948777. input_tokens=6, output_tokens=0
22:41:53,246 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4219999999986612. input_tokens=18, output_tokens=0
22:41:53,622 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,624 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.1630000000004657. input_tokens=29, output_tokens=0
22:41:53,646 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,649 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3880000000062864. input_tokens=10, output_tokens=0
22:41:53,961 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,964 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.271999999997206. input_tokens=22, output_tokens=0
22:41:53,970 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2789999999949941. input_tokens=5, output_tokens=0
22:41:53,976 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:53,979 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.002999999996973. input_tokens=30, output_tokens=0
22:41:54,240 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
22:41:54,241 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['MEXICO:']}
22:41:54,598 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:54,601 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8720000000030268. input_tokens=3, output_tokens=0
22:41:54,827 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:54,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.1410000000032596. input_tokens=27, output_tokens=0
22:41:56,311 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:41:56,313 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 1 retries took 0.34399999999732245. input_tokens=4, output_tokens=0
22:41:56,326 datashaper.workflow.workflow INFO executing verb drop
22:41:56,332 datashaper.workflow.workflow INFO executing verb filter
22:41:56,340 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:41:56,470 graphrag.index.run INFO Running workflow: create_final_nodes...
22:41:56,475 graphrag.index.run INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:41:56,476 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
22:41:56,490 datashaper.workflow.workflow INFO executing verb layout_graph
22:41:56,529 datashaper.workflow.workflow INFO executing verb unpack_graph
22:41:56,546 datashaper.workflow.workflow INFO executing verb unpack_graph
22:41:56,562 datashaper.workflow.workflow INFO executing verb drop
22:41:56,568 datashaper.workflow.workflow INFO executing verb filter
22:41:56,584 datashaper.workflow.workflow INFO executing verb select
22:41:56,591 datashaper.workflow.workflow INFO executing verb rename
22:41:56,597 datashaper.workflow.workflow INFO executing verb convert
22:41:56,618 datashaper.workflow.workflow INFO executing verb join
22:41:56,628 datashaper.workflow.workflow INFO executing verb rename
22:41:56,630 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:41:56,741 graphrag.index.run INFO Running workflow: create_final_communities...
22:41:56,741 graphrag.index.run INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:41:56,741 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
22:41:56,759 datashaper.workflow.workflow INFO executing verb unpack_graph
22:41:56,776 datashaper.workflow.workflow INFO executing verb unpack_graph
22:41:56,792 datashaper.workflow.workflow INFO executing verb aggregate_override
22:41:56,801 datashaper.workflow.workflow INFO executing verb join
22:41:56,812 datashaper.workflow.workflow INFO executing verb join
22:41:56,823 datashaper.workflow.workflow INFO executing verb concat
22:41:56,831 datashaper.workflow.workflow INFO executing verb filter
22:41:56,861 datashaper.workflow.workflow INFO executing verb aggregate_override
22:41:56,871 datashaper.workflow.workflow INFO executing verb join
22:41:56,882 datashaper.workflow.workflow INFO executing verb filter
22:41:56,902 datashaper.workflow.workflow INFO executing verb fill
22:41:56,911 datashaper.workflow.workflow INFO executing verb merge
22:41:56,923 datashaper.workflow.workflow INFO executing verb copy
22:41:56,932 datashaper.workflow.workflow INFO executing verb select
22:41:56,933 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:41:57,48 graphrag.index.run INFO Running workflow: join_text_units_to_entity_ids...
22:41:57,48 graphrag.index.run INFO dependencies for join_text_units_to_entity_ids: ['create_final_entities']
22:41:57,49 graphrag.index.run INFO read table from storage: create_final_entities.parquet
22:41:57,73 datashaper.workflow.workflow INFO executing verb select
22:41:57,82 datashaper.workflow.workflow INFO executing verb unroll
22:41:57,92 datashaper.workflow.workflow INFO executing verb aggregate_override
22:41:57,94 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_entity_ids.parquet
22:41:57,200 graphrag.index.run INFO Running workflow: create_final_relationships...
22:41:57,200 graphrag.index.run INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:41:57,200 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
22:41:57,203 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
22:41:57,224 datashaper.workflow.workflow INFO executing verb unpack_graph
22:41:57,243 datashaper.workflow.workflow INFO executing verb filter
22:41:57,265 datashaper.workflow.workflow INFO executing verb rename
22:41:57,275 datashaper.workflow.workflow INFO executing verb filter
22:41:57,298 datashaper.workflow.workflow INFO executing verb drop
22:41:57,308 datashaper.workflow.workflow INFO executing verb compute_edge_combined_degree
22:41:57,320 datashaper.workflow.workflow INFO executing verb convert
22:41:57,341 datashaper.workflow.workflow INFO executing verb convert
22:41:57,342 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:41:57,456 graphrag.index.run INFO Running workflow: join_text_units_to_relationship_ids...
22:41:57,456 graphrag.index.run INFO dependencies for join_text_units_to_relationship_ids: ['create_final_relationships']
22:41:57,456 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
22:41:57,479 datashaper.workflow.workflow INFO executing verb select
22:41:57,500 datashaper.workflow.workflow INFO executing verb unroll
22:41:57,512 datashaper.workflow.workflow INFO executing verb aggregate_override
22:41:57,524 datashaper.workflow.workflow INFO executing verb select
22:41:57,525 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_relationship_ids.parquet
22:41:57,635 graphrag.index.run INFO Running workflow: create_final_community_reports...
22:41:57,635 graphrag.index.run INFO dependencies for create_final_community_reports: ['create_final_covariates', 'create_final_nodes', 'create_final_relationships']
22:41:57,635 graphrag.index.run INFO read table from storage: create_final_covariates.parquet
22:41:57,638 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
22:41:57,641 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
22:41:57,664 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:41:57,677 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:41:57,691 datashaper.workflow.workflow INFO executing verb prepare_community_reports_claims
22:41:57,703 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:41:57,718 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:41:57,718 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 225
22:41:57,767 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 225
22:41:57,812 datashaper.workflow.workflow INFO executing verb create_community_reports
22:42:20,207 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:20,207 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:20,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.375. input_tokens=2147, output_tokens=355
22:42:22,660 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:22,660 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:22,660 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.812000000005355. input_tokens=2219, output_tokens=496
22:42:22,967 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:22,968 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:22,968 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.131000000001222. input_tokens=2022, output_tokens=556
22:42:23,795 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:23,795 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:23,796 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.96600000000035. input_tokens=2172, output_tokens=478
22:42:25,633 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:25,633 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:25,633 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 27.794000000001688. input_tokens=2022, output_tokens=517
22:42:26,40 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:26,41 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:26,41 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 28.215000000003783. input_tokens=2040, output_tokens=526
22:42:26,550 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:26,551 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:26,551 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 28.72899999999936. input_tokens=2074, output_tokens=509
22:42:27,472 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:27,472 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:27,473 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.60899999999674. input_tokens=2736, output_tokens=613
22:42:27,677 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:27,677 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:27,678 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:27,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.843000000000757. input_tokens=2193, output_tokens=545
22:42:27,678 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:27,679 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.8550000000032. input_tokens=2082, output_tokens=559
22:42:29,116 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:29,117 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:29,117 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 31.27599999999802. input_tokens=2008, output_tokens=531
22:42:29,628 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:29,629 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:29,629 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 31.760000000002037. input_tokens=2517, output_tokens=574
22:42:32,696 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:32,697 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:32,697 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 34.86499999999796. input_tokens=2247, output_tokens=691
22:42:33,211 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:33,212 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:33,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.35299999999552. input_tokens=2167, output_tokens=638
22:42:33,720 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:33,720 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:33,720 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.858000000000175. input_tokens=2226, output_tokens=530
22:42:34,436 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:34,436 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:34,437 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.57999999999447. input_tokens=2538, output_tokens=613
22:42:34,748 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:34,748 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:34,748 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:34,749 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.88900000000285. input_tokens=2207, output_tokens=559
22:42:34,749 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:34,749 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.882000000005064. input_tokens=2781, output_tokens=619
22:42:35,84 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:35,84 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:35,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.25699999999779. input_tokens=2259, output_tokens=526
22:42:35,591 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:35,591 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:35,591 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.741000000001804. input_tokens=2551, output_tokens=602
22:42:37,98 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:37,99 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:37,99 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.24399999999878. input_tokens=2762, output_tokens=685
22:42:38,19 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:38,20 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:38,20 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 40.17399999999907. input_tokens=3097, output_tokens=719
22:42:43,960 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:43,961 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:43,961 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 46.108000000000175. input_tokens=4135, output_tokens=868
22:42:45,803 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:42:45,804 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:42:45,804 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 47.95999999999913. input_tokens=7503, output_tokens=786
22:43:19,186 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:43:19,187 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:43:19,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 81.36699999999837. input_tokens=3917, output_tokens=590
22:43:43,865 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:43:43,865 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:43:43,866 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.652000000001863. input_tokens=2346, output_tokens=512
22:43:48,165 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:43:48,166 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:43:48,166 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 28.937000000005355. input_tokens=2460, output_tokens=606
22:43:51,441 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:43:51,442 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:43:51,442 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 32.21100000000297. input_tokens=2058, output_tokens=539
22:43:52,63 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:43:52,63 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:43:52,64 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 32.84599999999773. input_tokens=3025, output_tokens=625
22:43:57,283 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:43:57,284 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:43:57,284 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 38.078999999997905. input_tokens=4123, output_tokens=549
22:43:57,693 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:43:57,693 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:43:57,694 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 38.478000000002794. input_tokens=3352, output_tokens=640
22:43:58,308 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:43:58,308 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:43:58,309 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.09700000000157. input_tokens=3739, output_tokens=698
22:43:58,507 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:43:58,508 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:43:58,508 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.28500000000349. input_tokens=3184, output_tokens=690
22:44:00,862 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:44:00,863 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:44:00,863 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.63600000000588. input_tokens=3285, output_tokens=769
22:44:02,302 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:44:02,303 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:44:02,303 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 43.07800000000134. input_tokens=2180, output_tokens=528
22:44:18,988 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:44:18,988 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:44:18,989 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 59.76799999999639. input_tokens=4656, output_tokens=750
22:44:33,427 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:44:33,427 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:44:33,428 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 74.21899999999732. input_tokens=8677, output_tokens=1118
22:44:33,453 datashaper.workflow.workflow INFO executing verb window
22:44:33,454 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:44:33,592 graphrag.index.run INFO Running workflow: create_final_text_units...
22:44:33,592 graphrag.index.run INFO dependencies for create_final_text_units: ['create_base_text_units', 'join_text_units_to_covariate_ids', 'join_text_units_to_entity_ids', 'join_text_units_to_relationship_ids']
22:44:33,593 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
22:44:33,596 graphrag.index.run INFO read table from storage: join_text_units_to_covariate_ids.parquet
22:44:33,598 graphrag.index.run INFO read table from storage: join_text_units_to_entity_ids.parquet
22:44:33,600 graphrag.index.run INFO read table from storage: join_text_units_to_relationship_ids.parquet
22:44:33,624 datashaper.workflow.workflow INFO executing verb select
22:44:33,636 datashaper.workflow.workflow INFO executing verb rename
22:44:33,649 datashaper.workflow.workflow INFO executing verb join
22:44:33,663 datashaper.workflow.workflow INFO executing verb join
22:44:33,678 datashaper.workflow.workflow INFO executing verb join
22:44:33,692 datashaper.workflow.workflow INFO executing verb aggregate_override
22:44:33,706 datashaper.workflow.workflow INFO executing verb select
22:44:33,707 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:44:33,829 graphrag.index.run INFO Running workflow: create_base_documents...
22:44:33,829 graphrag.index.run INFO dependencies for create_base_documents: ['create_final_text_units']
22:44:33,829 graphrag.index.run INFO read table from storage: create_final_text_units.parquet
22:44:33,858 datashaper.workflow.workflow INFO executing verb unroll
22:44:33,871 datashaper.workflow.workflow INFO executing verb select
22:44:33,884 datashaper.workflow.workflow INFO executing verb rename
22:44:33,898 datashaper.workflow.workflow INFO executing verb join
22:44:33,913 datashaper.workflow.workflow INFO executing verb aggregate_override
22:44:33,927 datashaper.workflow.workflow INFO executing verb join
22:44:33,943 datashaper.workflow.workflow INFO executing verb rename
22:44:33,957 datashaper.workflow.workflow INFO executing verb convert
22:44:33,972 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
22:44:34,86 graphrag.index.run INFO Running workflow: create_final_documents...
22:44:34,86 graphrag.index.run INFO dependencies for create_final_documents: ['create_base_documents']
22:44:34,87 graphrag.index.run INFO read table from storage: create_base_documents.parquet
22:44:34,115 datashaper.workflow.workflow INFO executing verb rename
22:44:34,117 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
