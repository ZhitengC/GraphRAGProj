22:54:21,91 graphrag.config.read_dotenv INFO Loading pipeline .env file
22:54:21,93 graphrag.index.cli INFO using default configuration: {
    "llm": {
        "api_key": "REDACTED, length 51",
        "type": "openai_chat",
        "model": "glm-4",
        "max_tokens": 2000,
        "temperature": 0.95,
        "top_p": 0.7,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:3000/v1",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "./",
    "reporting": {
        "type": "file",
        "base_dir": "inputs/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "inputs/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_embedding",
            "model": "embedding-2",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 1,
        "batch_max_tokens": 8000,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.95,
            "top_p": 0.7,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:54:21,94 graphrag.index.create_pipeline_config INFO skipping workflows 
22:54:21,96 graphrag.index.run INFO Running pipeline
22:54:21,96 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at inputs/artifacts
22:54:21,96 graphrag.index.input.load_input INFO loading input from root_dir=input
22:54:21,96 graphrag.index.input.load_input INFO using file storage for input
22:54:21,97 graphrag.index.storage.file_pipeline_storage INFO search input for files matching .*\.txt$
22:54:21,97 graphrag.index.input.text INFO found text files from input, found [('currentPrompt.txt', {})]
22:54:21,98 graphrag.index.input.text INFO Found 1 files, loading 1
22:54:21,99 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_final_covariates', 'create_summarized_entities', 'join_text_units_to_covariate_ids', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
22:54:21,99 graphrag.index.run INFO Final # of rows loaded: 1
22:54:21,185 graphrag.index.run INFO Running workflow: create_base_text_units...
22:54:21,185 graphrag.index.run INFO dependencies for create_base_text_units: []
22:54:21,187 datashaper.workflow.workflow INFO executing verb orderby
22:54:21,188 datashaper.workflow.workflow INFO executing verb zip
22:54:21,190 datashaper.workflow.workflow INFO executing verb aggregate_override
22:54:21,193 datashaper.workflow.workflow INFO executing verb chunk
22:54:21,277 datashaper.workflow.workflow INFO executing verb select
22:54:21,279 datashaper.workflow.workflow INFO executing verb unroll
22:54:21,281 datashaper.workflow.workflow INFO executing verb rename
22:54:21,283 datashaper.workflow.workflow INFO executing verb genid
22:54:21,285 datashaper.workflow.workflow INFO executing verb unzip
22:54:21,288 datashaper.workflow.workflow INFO executing verb copy
22:54:21,290 datashaper.workflow.workflow INFO executing verb filter
22:54:21,295 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:54:21,385 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
22:54:21,385 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:54:21,386 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
22:54:21,393 datashaper.workflow.workflow INFO executing verb entity_extract
22:54:21,394 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:3000/v1
22:54:21,398 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for glm-4: TPM=0, RPM=0
22:54:21,398 graphrag.index.llm.load_llm INFO create concurrency limiter for glm-4: 25
22:54:21,407 datashaper.workflow.workflow INFO executing verb merge_graphs
22:54:21,410 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:54:21,499 graphrag.index.run INFO Running workflow: create_final_covariates...
22:54:21,499 graphrag.index.run INFO dependencies for create_final_covariates: ['create_base_text_units']
22:54:21,500 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
22:54:21,507 datashaper.workflow.workflow INFO executing verb extract_covariates
22:55:19,807 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:55:19,809 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 58.296000000002095. input_tokens=2315, output_tokens=1180
22:55:28,286 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:55:28,286 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 66.77100000000064. input_tokens=2175, output_tokens=1136
22:55:36,785 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:55:36,786 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 75.27700000000186. input_tokens=2314, output_tokens=1317
22:56:00,30 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:00,31 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.220000000001164. input_tokens=19, output_tokens=910
22:56:44,990 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:44,991 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 76.70300000000134. input_tokens=19, output_tokens=1656
22:56:50,412 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:50,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 73.625. input_tokens=19, output_tokens=1571
22:56:50,419 datashaper.workflow.workflow INFO executing verb window
22:56:50,422 datashaper.workflow.workflow INFO executing verb genid
22:56:50,426 datashaper.workflow.workflow INFO executing verb convert
22:56:50,432 datashaper.workflow.workflow INFO executing verb rename
22:56:50,436 datashaper.workflow.workflow INFO executing verb select
22:56:50,436 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_covariates.parquet
22:56:50,544 graphrag.index.run INFO Running workflow: create_summarized_entities...
22:56:50,544 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:56:50,544 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
22:56:50,552 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:56:52,859 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:52,859 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.290999999997439. input_tokens=140, output_tokens=20
22:56:53,176 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:53,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6160000000018044. input_tokens=158, output_tokens=52
22:56:53,190 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:53,190 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.625. input_tokens=155, output_tokens=33
22:56:53,603 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:53,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.040000000000873. input_tokens=154, output_tokens=32
22:56:53,696 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:53,696 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.125. input_tokens=172, output_tokens=44
22:56:54,101 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:54,102 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.536000000000058. input_tokens=160, output_tokens=57
22:56:54,419 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:54,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.84599999999773. input_tokens=152, output_tokens=69
22:56:54,420 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:54,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.8620000000009895. input_tokens=213, output_tokens=92
22:56:54,612 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:54,612 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.05000000000291. input_tokens=155, output_tokens=44
22:56:54,924 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:54,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.357000000003609. input_tokens=156, output_tokens=36
22:56:56,149 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:56,149 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:56,150 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.5789999999979045. input_tokens=145, output_tokens=27
22:56:56,150 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.576999999997497. input_tokens=160, output_tokens=55
22:56:56,760 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:56:56,761 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.193999999995867. input_tokens=167, output_tokens=52
22:56:56,767 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:56:56,861 graphrag.index.run INFO Running workflow: join_text_units_to_covariate_ids...
22:56:56,861 graphrag.index.run INFO dependencies for join_text_units_to_covariate_ids: ['create_final_covariates']
22:56:56,861 graphrag.index.run INFO read table from storage: create_final_covariates.parquet
22:56:56,872 datashaper.workflow.workflow INFO executing verb select
22:56:56,876 datashaper.workflow.workflow INFO executing verb aggregate_override
22:56:56,878 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_covariate_ids.parquet
22:56:56,978 graphrag.index.run INFO Running workflow: create_base_entity_graph...
22:56:56,978 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:56:56,979 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
22:56:56,989 datashaper.workflow.workflow INFO executing verb cluster_graph
22:56:57,1 datashaper.workflow.workflow INFO executing verb select
22:56:57,3 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:56:57,94 graphrag.index.run INFO Running workflow: create_final_entities...
22:56:57,94 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:56:57,94 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
22:56:57,105 datashaper.workflow.workflow INFO executing verb unpack_graph
22:56:57,112 datashaper.workflow.workflow INFO executing verb rename
22:56:57,117 datashaper.workflow.workflow INFO executing verb select
22:56:57,122 datashaper.workflow.workflow INFO executing verb dedupe
22:56:57,127 datashaper.workflow.workflow INFO executing verb rename
22:56:57,132 datashaper.workflow.workflow INFO executing verb filter
22:56:57,143 datashaper.workflow.workflow INFO executing verb text_split
22:56:57,149 datashaper.workflow.workflow INFO executing verb drop
22:56:57,154 datashaper.workflow.workflow INFO executing verb merge
22:56:57,164 datashaper.workflow.workflow INFO executing verb text_embed
22:56:57,165 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:3000/v1
22:56:57,169 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for embedding-2: TPM=0, RPM=0
22:56:57,169 graphrag.index.llm.load_llm INFO create concurrency limiter for embedding-2: 25
22:56:57,170 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 55 inputs via 55 snippets using 55 batches. max_batch_size=1, max_tokens=8000
22:56:57,540 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,543 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.36899999999877764. input_tokens=104, output_tokens=0
22:56:57,543 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3739999999961583. input_tokens=49, output_tokens=0
22:56:57,547 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,549 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.375. input_tokens=22, output_tokens=0
22:56:57,592 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,594 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4180000000051223. input_tokens=14, output_tokens=0
22:56:57,600 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,603 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4279999999998836. input_tokens=21, output_tokens=0
22:56:57,725 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,725 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,727 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5509999999994761. input_tokens=64, output_tokens=0
22:56:57,729 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5529999999998836. input_tokens=22, output_tokens=0
22:56:57,731 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,731 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,731 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,731 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,733 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5600000000049477. input_tokens=38, output_tokens=0
22:56:57,736 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.559000000001106. input_tokens=14, output_tokens=0
22:56:57,738 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5640000000057626. input_tokens=33, output_tokens=0
22:56:57,740 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5629999999946449. input_tokens=41, output_tokens=0
22:56:57,752 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,754 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5769999999974971. input_tokens=29, output_tokens=0
22:56:57,798 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,799 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,801 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.625. input_tokens=52, output_tokens=0
22:56:57,803 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6290000000008149. input_tokens=14, output_tokens=0
22:56:57,809 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6350000000020373. input_tokens=35, output_tokens=0
22:56:57,812 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,814 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6399999999994179. input_tokens=19, output_tokens=0
22:56:57,954 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,957 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.36200000000098953. input_tokens=3, output_tokens=0
22:56:57,966 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:57,968 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4239999999990687. input_tokens=19, output_tokens=0
22:56:58,29 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,31 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4279999999998836. input_tokens=16, output_tokens=0
22:56:58,174 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,174 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4459999999962747. input_tokens=34, output_tokens=0
22:56:58,178 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3739999999961583. input_tokens=23, output_tokens=0
22:56:58,233 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,235 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6880000000019209. input_tokens=24, output_tokens=0
22:56:58,244 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5160000000032596. input_tokens=20, output_tokens=0
22:56:58,386 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,388 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2139999999999418. input_tokens=60, output_tokens=0
22:56:58,389 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,389 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,390 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,390 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,390 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,390 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,390 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,392 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6510000000052969. input_tokens=28, output_tokens=0
22:56:58,394 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5800000000017462. input_tokens=20, output_tokens=0
22:56:58,397 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6419999999998254. input_tokens=18, output_tokens=0
22:56:58,399 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5950000000011642. input_tokens=21, output_tokens=0
22:56:58,401 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5889999999999418. input_tokens=16, output_tokens=0
22:56:58,403 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8539999999993597. input_tokens=20, output_tokens=0
22:56:58,406 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.44900000000052387. input_tokens=58, output_tokens=0
22:56:58,457 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,457 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,457 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,459 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7189999999973224. input_tokens=21, output_tokens=0
22:56:58,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7200000000011642. input_tokens=19, output_tokens=0
22:56:58,464 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7229999999981374. input_tokens=14, output_tokens=0
22:56:58,556 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3830000000016298. input_tokens=18, output_tokens=0
22:56:58,559 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,567 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5350000000034925. input_tokens=20, output_tokens=0
22:56:58,568 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.39099999999598367. input_tokens=32, output_tokens=0
22:56:58,605 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.42899999999644933. input_tokens=21, output_tokens=0
22:56:58,640 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,643 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3960000000006403. input_tokens=25, output_tokens=0
22:56:58,654 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,656 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.41999999999825377. input_tokens=29, output_tokens=0
22:56:58,708 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5350000000034925. input_tokens=15, output_tokens=0
22:56:58,747 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,750 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3609999999971478. input_tokens=47, output_tokens=0
22:56:58,773 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,773 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,773 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3669999999983702. input_tokens=32, output_tokens=0
22:56:58,778 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3699999999953434. input_tokens=27, output_tokens=0
22:56:58,780 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.37299999999959255. input_tokens=21, output_tokens=0
22:56:58,781 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,781 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,783 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.375. input_tokens=20, output_tokens=0
22:56:58,786 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3790000000008149. input_tokens=57, output_tokens=0
22:56:58,794 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,794 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,796 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6199999999953434. input_tokens=15, output_tokens=0
22:56:58,799 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6239999999961583. input_tokens=19, output_tokens=0
22:56:58,812 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,815 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.408000000003085. input_tokens=27, output_tokens=0
22:56:58,921 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,921 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:58,924 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7479999999995925. input_tokens=55, output_tokens=0
22:56:58,926 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7509999999965657. input_tokens=30, output_tokens=0
22:56:59,6 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:59,6 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:59,8 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8319999999948777. input_tokens=37, output_tokens=0
22:56:59,11 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8359999999956926. input_tokens=21, output_tokens=0
22:56:59,136 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 200 OK"
22:56:59,138 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.168999999994412. input_tokens=24, output_tokens=0
22:56:59,150 datashaper.workflow.workflow INFO executing verb drop
22:56:59,156 datashaper.workflow.workflow INFO executing verb filter
22:56:59,164 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:56:59,300 graphrag.index.run INFO Running workflow: create_final_nodes...
22:56:59,300 graphrag.index.run INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:56:59,300 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
22:56:59,314 datashaper.workflow.workflow INFO executing verb layout_graph
22:56:59,330 datashaper.workflow.workflow INFO executing verb unpack_graph
22:56:59,340 datashaper.workflow.workflow INFO executing verb unpack_graph
22:56:59,352 datashaper.workflow.workflow INFO executing verb filter
22:56:59,369 datashaper.workflow.workflow INFO executing verb drop
22:56:59,376 datashaper.workflow.workflow INFO executing verb select
22:56:59,383 datashaper.workflow.workflow INFO executing verb rename
22:56:59,391 datashaper.workflow.workflow INFO executing verb convert
22:56:59,412 datashaper.workflow.workflow INFO executing verb join
22:56:59,422 datashaper.workflow.workflow INFO executing verb rename
22:56:59,423 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:56:59,533 graphrag.index.run INFO Running workflow: create_final_communities...
22:56:59,533 graphrag.index.run INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:56:59,533 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
22:56:59,549 datashaper.workflow.workflow INFO executing verb unpack_graph
22:56:59,559 datashaper.workflow.workflow INFO executing verb unpack_graph
22:56:59,569 datashaper.workflow.workflow INFO executing verb aggregate_override
22:56:59,578 datashaper.workflow.workflow INFO executing verb join
22:56:59,589 datashaper.workflow.workflow INFO executing verb join
22:56:59,600 datashaper.workflow.workflow INFO executing verb concat
22:56:59,609 datashaper.workflow.workflow INFO executing verb filter
22:56:59,641 datashaper.workflow.workflow INFO executing verb aggregate_override
22:56:59,652 datashaper.workflow.workflow INFO executing verb join
22:56:59,664 datashaper.workflow.workflow INFO executing verb filter
22:56:59,683 datashaper.workflow.workflow INFO executing verb fill
22:56:59,692 datashaper.workflow.workflow INFO executing verb merge
22:56:59,701 datashaper.workflow.workflow INFO executing verb copy
22:56:59,710 datashaper.workflow.workflow INFO executing verb select
22:56:59,711 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:56:59,822 graphrag.index.run INFO Running workflow: join_text_units_to_entity_ids...
22:56:59,822 graphrag.index.run INFO dependencies for join_text_units_to_entity_ids: ['create_final_entities']
22:56:59,822 graphrag.index.run INFO read table from storage: create_final_entities.parquet
22:56:59,846 datashaper.workflow.workflow INFO executing verb select
22:56:59,855 datashaper.workflow.workflow INFO executing verb unroll
22:56:59,865 datashaper.workflow.workflow INFO executing verb aggregate_override
22:56:59,867 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_entity_ids.parquet
22:56:59,974 graphrag.index.run INFO Running workflow: create_final_relationships...
22:56:59,974 graphrag.index.run INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
22:56:59,974 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
22:56:59,978 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
22:56:59,997 datashaper.workflow.workflow INFO executing verb unpack_graph
22:57:00,10 datashaper.workflow.workflow INFO executing verb filter
22:57:00,29 datashaper.workflow.workflow INFO executing verb rename
22:57:00,39 datashaper.workflow.workflow INFO executing verb filter
22:57:00,60 datashaper.workflow.workflow INFO executing verb drop
22:57:00,70 datashaper.workflow.workflow INFO executing verb compute_edge_combined_degree
22:57:00,81 datashaper.workflow.workflow INFO executing verb convert
22:57:00,101 datashaper.workflow.workflow INFO executing verb convert
22:57:00,103 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:57:00,223 graphrag.index.run INFO Running workflow: join_text_units_to_relationship_ids...
22:57:00,223 graphrag.index.run INFO dependencies for join_text_units_to_relationship_ids: ['create_final_relationships']
22:57:00,224 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
22:57:00,246 datashaper.workflow.workflow INFO executing verb select
22:57:00,257 datashaper.workflow.workflow INFO executing verb unroll
22:57:00,267 datashaper.workflow.workflow INFO executing verb aggregate_override
22:57:00,279 datashaper.workflow.workflow INFO executing verb select
22:57:00,280 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_relationship_ids.parquet
22:57:00,391 graphrag.index.run INFO Running workflow: create_final_community_reports...
22:57:00,391 graphrag.index.run INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships', 'create_final_covariates']
22:57:00,391 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
22:57:00,395 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
22:57:00,397 graphrag.index.run INFO read table from storage: create_final_covariates.parquet
22:57:00,420 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:57:00,432 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:57:00,444 datashaper.workflow.workflow INFO executing verb prepare_community_reports_claims
22:57:00,455 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:57:00,468 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:57:00,468 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 55
22:57:00,479 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 55
22:57:00,503 datashaper.workflow.workflow INFO executing verb create_community_reports
22:57:28,302 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:57:28,302 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:57:28,303 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 27.794999999998254. input_tokens=2806, output_tokens=623
22:57:35,687 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:57:35,687 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:57:35,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.1820000000007. input_tokens=2341, output_tokens=584
22:57:58,821 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:57:58,822 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:57:58,822 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.11699999999837. input_tokens=2431, output_tokens=496
22:58:08,136 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:58:08,136 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:58:08,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 32.434000000001106. input_tokens=3155, output_tokens=688
22:58:11,520 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
22:58:11,521 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
22:58:11,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.81399999999849. input_tokens=3574, output_tokens=813
22:58:11,545 datashaper.workflow.workflow INFO executing verb window
22:58:11,547 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:58:11,665 graphrag.index.run INFO Running workflow: create_final_text_units...
22:58:11,665 graphrag.index.run INFO dependencies for create_final_text_units: ['create_base_text_units', 'join_text_units_to_entity_ids', 'join_text_units_to_covariate_ids', 'join_text_units_to_relationship_ids']
22:58:11,665 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
22:58:11,668 graphrag.index.run INFO read table from storage: join_text_units_to_entity_ids.parquet
22:58:11,670 graphrag.index.run INFO read table from storage: join_text_units_to_covariate_ids.parquet
22:58:11,671 graphrag.index.run INFO read table from storage: join_text_units_to_relationship_ids.parquet
22:58:11,696 datashaper.workflow.workflow INFO executing verb select
22:58:11,708 datashaper.workflow.workflow INFO executing verb rename
22:58:11,720 datashaper.workflow.workflow INFO executing verb join
22:58:11,735 datashaper.workflow.workflow INFO executing verb join
22:58:11,749 datashaper.workflow.workflow INFO executing verb join
22:58:11,764 datashaper.workflow.workflow INFO executing verb aggregate_override
22:58:11,778 datashaper.workflow.workflow INFO executing verb select
22:58:11,779 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:58:11,892 graphrag.index.run INFO Running workflow: create_base_documents...
22:58:11,892 graphrag.index.run INFO dependencies for create_base_documents: ['create_final_text_units']
22:58:11,893 graphrag.index.run INFO read table from storage: create_final_text_units.parquet
22:58:11,920 datashaper.workflow.workflow INFO executing verb unroll
22:58:11,934 datashaper.workflow.workflow INFO executing verb select
22:58:11,947 datashaper.workflow.workflow INFO executing verb rename
22:58:11,959 datashaper.workflow.workflow INFO executing verb join
22:58:11,974 datashaper.workflow.workflow INFO executing verb aggregate_override
22:58:11,988 datashaper.workflow.workflow INFO executing verb join
22:58:12,3 datashaper.workflow.workflow INFO executing verb rename
22:58:12,17 datashaper.workflow.workflow INFO executing verb convert
22:58:12,31 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
22:58:12,145 graphrag.index.run INFO Running workflow: create_final_documents...
22:58:12,145 graphrag.index.run INFO dependencies for create_final_documents: ['create_base_documents']
22:58:12,145 graphrag.index.run INFO read table from storage: create_base_documents.parquet
22:58:12,175 datashaper.workflow.workflow INFO executing verb rename
22:58:12,176 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
